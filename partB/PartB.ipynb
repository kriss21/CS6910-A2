{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b52a678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02f03c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0b72f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9003 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n",
      "Found 2000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "pref = '../inaturalist_12K/'\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    pref+'train',\n",
    "    target_size=(224,224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical')\n",
    "\n",
    "val_generator = test_datagen.flow_from_directory(\n",
    "    pref+'val',\n",
    "    target_size=(224,224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    pref+'test',\n",
    "    target_size=(224,224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b29d6c",
   "metadata": {},
   "source": [
    "# Question 1 : Implementing flexible model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe9ef180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from keras.layers import Activation, Dense, Flatten \n",
    "\n",
    "class my_model:\n",
    "    def __init__(self , pretrained_model , input_shape, unfreeze_from):\n",
    "        unfreeze = int(unfreeze_from)\n",
    "        self.input_shape = input_shape\n",
    "        self.model = Sequential()\n",
    "        pretrained = (pretrained_model(weights = \"imagenet\", include_top = False, input_shape=input_shape))\n",
    "        i = 0\n",
    "        for layers in pretrained.layers[ : (unfreeze)] :\n",
    "            layers.trainable = False\n",
    "        for layers in pretrained.layers :\n",
    "            print(i+1, \" \" ,layers.name, \" \", layers.trainable)\n",
    "            i = i+1\n",
    "        self.model.add(pretrained)\n",
    "        \n",
    "    def add_dense_layer(self,neurons=10):\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(neurons,activation='relu'))\n",
    "        \n",
    "    def build_model(self,output,optimizer='adam',loss='categorical_crossentropy'):\n",
    "        self.model.add(Dense(output,activation='softmax'))\n",
    "        self.model.compile(optimizer=optimizer,loss=loss,metrics=['accuracy'])\n",
    "        self.model.summary()\n",
    "        return self.model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8ed8422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   input_1   False\n",
      "2   conv1_pad   False\n",
      "3   conv1_conv   False\n",
      "4   conv1_bn   False\n",
      "5   conv1_relu   False\n",
      "6   pool1_pad   False\n",
      "7   pool1_pool   False\n",
      "8   conv2_block1_1_conv   False\n",
      "9   conv2_block1_1_bn   False\n",
      "10   conv2_block1_1_relu   False\n",
      "11   conv2_block1_2_conv   False\n",
      "12   conv2_block1_2_bn   False\n",
      "13   conv2_block1_2_relu   False\n",
      "14   conv2_block1_0_conv   False\n",
      "15   conv2_block1_3_conv   False\n",
      "16   conv2_block1_0_bn   False\n",
      "17   conv2_block1_3_bn   False\n",
      "18   conv2_block1_add   False\n",
      "19   conv2_block1_out   False\n",
      "20   conv2_block2_1_conv   False\n",
      "21   conv2_block2_1_bn   False\n",
      "22   conv2_block2_1_relu   False\n",
      "23   conv2_block2_2_conv   False\n",
      "24   conv2_block2_2_bn   False\n",
      "25   conv2_block2_2_relu   False\n",
      "26   conv2_block2_3_conv   False\n",
      "27   conv2_block2_3_bn   False\n",
      "28   conv2_block2_add   False\n",
      "29   conv2_block2_out   False\n",
      "30   conv2_block3_1_conv   False\n",
      "31   conv2_block3_1_bn   False\n",
      "32   conv2_block3_1_relu   False\n",
      "33   conv2_block3_2_conv   False\n",
      "34   conv2_block3_2_bn   False\n",
      "35   conv2_block3_2_relu   False\n",
      "36   conv2_block3_3_conv   False\n",
      "37   conv2_block3_3_bn   False\n",
      "38   conv2_block3_add   False\n",
      "39   conv2_block3_out   False\n",
      "40   conv3_block1_1_conv   False\n",
      "41   conv3_block1_1_bn   False\n",
      "42   conv3_block1_1_relu   False\n",
      "43   conv3_block1_2_conv   False\n",
      "44   conv3_block1_2_bn   False\n",
      "45   conv3_block1_2_relu   False\n",
      "46   conv3_block1_0_conv   False\n",
      "47   conv3_block1_3_conv   False\n",
      "48   conv3_block1_0_bn   False\n",
      "49   conv3_block1_3_bn   False\n",
      "50   conv3_block1_add   False\n",
      "51   conv3_block1_out   False\n",
      "52   conv3_block2_1_conv   False\n",
      "53   conv3_block2_1_bn   False\n",
      "54   conv3_block2_1_relu   False\n",
      "55   conv3_block2_2_conv   False\n",
      "56   conv3_block2_2_bn   False\n",
      "57   conv3_block2_2_relu   False\n",
      "58   conv3_block2_3_conv   False\n",
      "59   conv3_block2_3_bn   False\n",
      "60   conv3_block2_add   False\n",
      "61   conv3_block2_out   False\n",
      "62   conv3_block3_1_conv   False\n",
      "63   conv3_block3_1_bn   False\n",
      "64   conv3_block3_1_relu   False\n",
      "65   conv3_block3_2_conv   False\n",
      "66   conv3_block3_2_bn   False\n",
      "67   conv3_block3_2_relu   False\n",
      "68   conv3_block3_3_conv   False\n",
      "69   conv3_block3_3_bn   False\n",
      "70   conv3_block3_add   False\n",
      "71   conv3_block3_out   False\n",
      "72   conv3_block4_1_conv   False\n",
      "73   conv3_block4_1_bn   False\n",
      "74   conv3_block4_1_relu   False\n",
      "75   conv3_block4_2_conv   False\n",
      "76   conv3_block4_2_bn   False\n",
      "77   conv3_block4_2_relu   False\n",
      "78   conv3_block4_3_conv   False\n",
      "79   conv3_block4_3_bn   False\n",
      "80   conv3_block4_add   False\n",
      "81   conv3_block4_out   False\n",
      "82   conv4_block1_1_conv   False\n",
      "83   conv4_block1_1_bn   False\n",
      "84   conv4_block1_1_relu   False\n",
      "85   conv4_block1_2_conv   False\n",
      "86   conv4_block1_2_bn   False\n",
      "87   conv4_block1_2_relu   False\n",
      "88   conv4_block1_0_conv   False\n",
      "89   conv4_block1_3_conv   False\n",
      "90   conv4_block1_0_bn   False\n",
      "91   conv4_block1_3_bn   False\n",
      "92   conv4_block1_add   False\n",
      "93   conv4_block1_out   False\n",
      "94   conv4_block2_1_conv   False\n",
      "95   conv4_block2_1_bn   False\n",
      "96   conv4_block2_1_relu   False\n",
      "97   conv4_block2_2_conv   False\n",
      "98   conv4_block2_2_bn   False\n",
      "99   conv4_block2_2_relu   False\n",
      "100   conv4_block2_3_conv   False\n",
      "101   conv4_block2_3_bn   False\n",
      "102   conv4_block2_add   False\n",
      "103   conv4_block2_out   False\n",
      "104   conv4_block3_1_conv   False\n",
      "105   conv4_block3_1_bn   False\n",
      "106   conv4_block3_1_relu   False\n",
      "107   conv4_block3_2_conv   False\n",
      "108   conv4_block3_2_bn   False\n",
      "109   conv4_block3_2_relu   False\n",
      "110   conv4_block3_3_conv   False\n",
      "111   conv4_block3_3_bn   False\n",
      "112   conv4_block3_add   False\n",
      "113   conv4_block3_out   False\n",
      "114   conv4_block4_1_conv   False\n",
      "115   conv4_block4_1_bn   False\n",
      "116   conv4_block4_1_relu   False\n",
      "117   conv4_block4_2_conv   False\n",
      "118   conv4_block4_2_bn   False\n",
      "119   conv4_block4_2_relu   False\n",
      "120   conv4_block4_3_conv   False\n",
      "121   conv4_block4_3_bn   False\n",
      "122   conv4_block4_add   False\n",
      "123   conv4_block4_out   False\n",
      "124   conv4_block5_1_conv   False\n",
      "125   conv4_block5_1_bn   False\n",
      "126   conv4_block5_1_relu   False\n",
      "127   conv4_block5_2_conv   False\n",
      "128   conv4_block5_2_bn   False\n",
      "129   conv4_block5_2_relu   False\n",
      "130   conv4_block5_3_conv   False\n",
      "131   conv4_block5_3_bn   False\n",
      "132   conv4_block5_add   False\n",
      "133   conv4_block5_out   False\n",
      "134   conv4_block6_1_conv   False\n",
      "135   conv4_block6_1_bn   False\n",
      "136   conv4_block6_1_relu   False\n",
      "137   conv4_block6_2_conv   False\n",
      "138   conv4_block6_2_bn   False\n",
      "139   conv4_block6_2_relu   False\n",
      "140   conv4_block6_3_conv   False\n",
      "141   conv4_block6_3_bn   False\n",
      "142   conv4_block6_add   False\n",
      "143   conv4_block6_out   False\n",
      "144   conv5_block1_1_conv   False\n",
      "145   conv5_block1_1_bn   False\n",
      "146   conv5_block1_1_relu   False\n",
      "147   conv5_block1_2_conv   False\n",
      "148   conv5_block1_2_bn   False\n",
      "149   conv5_block1_2_relu   False\n",
      "150   conv5_block1_0_conv   False\n",
      "151   conv5_block1_3_conv   False\n",
      "152   conv5_block1_0_bn   False\n",
      "153   conv5_block1_3_bn   False\n",
      "154   conv5_block1_add   False\n",
      "155   conv5_block1_out   False\n",
      "156   conv5_block2_1_conv   True\n",
      "157   conv5_block2_1_bn   True\n",
      "158   conv5_block2_1_relu   True\n",
      "159   conv5_block2_2_conv   True\n",
      "160   conv5_block2_2_bn   True\n",
      "161   conv5_block2_2_relu   True\n",
      "162   conv5_block2_3_conv   True\n",
      "163   conv5_block2_3_bn   True\n",
      "164   conv5_block2_add   True\n",
      "165   conv5_block2_out   True\n",
      "166   conv5_block3_1_conv   True\n",
      "167   conv5_block3_1_bn   True\n",
      "168   conv5_block3_1_relu   True\n",
      "169   conv5_block3_2_conv   True\n",
      "170   conv5_block3_2_bn   True\n",
      "171   conv5_block3_2_relu   True\n",
      "172   conv5_block3_3_conv   True\n",
      "173   conv5_block3_3_bn   True\n",
      "174   conv5_block3_add   True\n",
      "175   conv5_block3_out   True\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Functional)        (None, 7, 7, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 7, 7, 10)          20490     \n",
      "=================================================================\n",
      "Total params: 23,608,202\n",
      "Trainable params: 8,951,818\n",
      "Non-trainable params: 14,656,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = my_model(ResNet50,(224,224,3),155)\n",
    "model1 = model.build_model(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd60ab6",
   "metadata": {},
   "source": [
    "# WandB Sweep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e7fe530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aea493ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid', #grid, random\n",
    "    'metric': {\n",
    "      'name': 'val_accuracy',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'model': {\n",
    "            'values': [ ('ResNet50',174,0) , ('ResNet50',165,1) , ('ResNet50',155,2),('InceptionV3',310,0),('InceptionV3',280,1), ('InceptionV3',249,2) ,('InceptionResNetV2',779,0), ('InceptionResNetV2',742,1),('InceptionResNetV2',710,2) , ('VGG16',18,0), ('VGG16',15,1),('VGG16',11,2)]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52ee90d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: wyy702q1\n",
      "Sweep URL: https://wandb.ai/notarchana/cs6910-a2/sweeps/wyy702q1\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config,entity = \"notarchana\" , project = \"cs6910-a2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6df853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(name) :\n",
    "    if name == \"VGG16\" :\n",
    "        return VGG16\n",
    "    if name == \"ResNet50\" :\n",
    "        return ResNet50\n",
    "    if name == \"InceptionV3\" :\n",
    "        return InceptionV3\n",
    "    if name == \"InceptionResNetV2\" :\n",
    "        return InceptionResNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e135e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    config_defaults = {\n",
    "        'model' : (ResNet50,174)\n",
    "    }\n",
    "    wandb.init(config=config_defaults,name=\"cs6910-a2-partB\")\n",
    "    cfg = wandb.config\n",
    "    wandb.run.name = cfg.model[0]+ \"_\" + str(cfg.model[2]) \n",
    "    wandb.run.save()\n",
    "    # Config is a variable that holds and saves hyperparameters and inputs\n",
    "    print(cfg.model[1])\n",
    "    model_functor = get_model(cfg.model[0])\n",
    "    model = my_model( model_functor, (224,224,3) , ((cfg.model)[1]) )\n",
    "    print(\"model retrieved\")\n",
    "    model.add_dense_layer(1024)\n",
    "    model.add_dense_layer(1024)\n",
    "    model.add_dense_layer(1024)\n",
    "    model = model.build_model(10)\n",
    "    \n",
    "    print(\"model building done\")\n",
    "    trained = model.fit(train_generator,\n",
    "                steps_per_epoch=80,\n",
    "                epochs=10 ,\n",
    "                batch_size = 64,\n",
    "                validation_data=val_generator,\n",
    "                validation_steps=10,\n",
    "                callbacks=[WandbCallback(monitor='val_accuracy',mode='max')]\n",
    "    )\n",
    "    \n",
    "    loss, acc = model.evaluate(test_generator)\n",
    "    wandb.log({'test_acc ' : acc , 'test_loss ' : loss})\n",
    "    \n",
    "\n",
    "    print(\"model training done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efd1d31c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: loj2oxgh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: ['ResNet50', 174, 0]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnotarchana\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.25<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">cs6910-a2-partB</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/notarchana/cs6910-a2\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/notarchana/cs6910-a2/sweeps/wyy702q1\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/sweeps/wyy702q1</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/notarchana/cs6910-a2/runs/loj2oxgh\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/runs/loj2oxgh</a><br/>\n",
       "                Run data is saved locally in <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210412_215813-loj2oxgh</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174\n",
      "1   input_1   False\n",
      "2   conv1_pad   False\n",
      "3   conv1_conv   False\n",
      "4   conv1_bn   False\n",
      "5   conv1_relu   False\n",
      "6   pool1_pad   False\n",
      "7   pool1_pool   False\n",
      "8   conv2_block1_1_conv   False\n",
      "9   conv2_block1_1_bn   False\n",
      "10   conv2_block1_1_relu   False\n",
      "11   conv2_block1_2_conv   False\n",
      "12   conv2_block1_2_bn   False\n",
      "13   conv2_block1_2_relu   False\n",
      "14   conv2_block1_0_conv   False\n",
      "15   conv2_block1_3_conv   False\n",
      "16   conv2_block1_0_bn   False\n",
      "17   conv2_block1_3_bn   False\n",
      "18   conv2_block1_add   False\n",
      "19   conv2_block1_out   False\n",
      "20   conv2_block2_1_conv   False\n",
      "21   conv2_block2_1_bn   False\n",
      "22   conv2_block2_1_relu   False\n",
      "23   conv2_block2_2_conv   False\n",
      "24   conv2_block2_2_bn   False\n",
      "25   conv2_block2_2_relu   False\n",
      "26   conv2_block2_3_conv   False\n",
      "27   conv2_block2_3_bn   False\n",
      "28   conv2_block2_add   False\n",
      "29   conv2_block2_out   False\n",
      "30   conv2_block3_1_conv   False\n",
      "31   conv2_block3_1_bn   False\n",
      "32   conv2_block3_1_relu   False\n",
      "33   conv2_block3_2_conv   False\n",
      "34   conv2_block3_2_bn   False\n",
      "35   conv2_block3_2_relu   False\n",
      "36   conv2_block3_3_conv   False\n",
      "37   conv2_block3_3_bn   False\n",
      "38   conv2_block3_add   False\n",
      "39   conv2_block3_out   False\n",
      "40   conv3_block1_1_conv   False\n",
      "41   conv3_block1_1_bn   False\n",
      "42   conv3_block1_1_relu   False\n",
      "43   conv3_block1_2_conv   False\n",
      "44   conv3_block1_2_bn   False\n",
      "45   conv3_block1_2_relu   False\n",
      "46   conv3_block1_0_conv   False\n",
      "47   conv3_block1_3_conv   False\n",
      "48   conv3_block1_0_bn   False\n",
      "49   conv3_block1_3_bn   False\n",
      "50   conv3_block1_add   False\n",
      "51   conv3_block1_out   False\n",
      "52   conv3_block2_1_conv   False\n",
      "53   conv3_block2_1_bn   False\n",
      "54   conv3_block2_1_relu   False\n",
      "55   conv3_block2_2_conv   False\n",
      "56   conv3_block2_2_bn   False\n",
      "57   conv3_block2_2_relu   False\n",
      "58   conv3_block2_3_conv   False\n",
      "59   conv3_block2_3_bn   False\n",
      "60   conv3_block2_add   False\n",
      "61   conv3_block2_out   False\n",
      "62   conv3_block3_1_conv   False\n",
      "63   conv3_block3_1_bn   False\n",
      "64   conv3_block3_1_relu   False\n",
      "65   conv3_block3_2_conv   False\n",
      "66   conv3_block3_2_bn   False\n",
      "67   conv3_block3_2_relu   False\n",
      "68   conv3_block3_3_conv   False\n",
      "69   conv3_block3_3_bn   False\n",
      "70   conv3_block3_add   False\n",
      "71   conv3_block3_out   False\n",
      "72   conv3_block4_1_conv   False\n",
      "73   conv3_block4_1_bn   False\n",
      "74   conv3_block4_1_relu   False\n",
      "75   conv3_block4_2_conv   False\n",
      "76   conv3_block4_2_bn   False\n",
      "77   conv3_block4_2_relu   False\n",
      "78   conv3_block4_3_conv   False\n",
      "79   conv3_block4_3_bn   False\n",
      "80   conv3_block4_add   False\n",
      "81   conv3_block4_out   False\n",
      "82   conv4_block1_1_conv   False\n",
      "83   conv4_block1_1_bn   False\n",
      "84   conv4_block1_1_relu   False\n",
      "85   conv4_block1_2_conv   False\n",
      "86   conv4_block1_2_bn   False\n",
      "87   conv4_block1_2_relu   False\n",
      "88   conv4_block1_0_conv   False\n",
      "89   conv4_block1_3_conv   False\n",
      "90   conv4_block1_0_bn   False\n",
      "91   conv4_block1_3_bn   False\n",
      "92   conv4_block1_add   False\n",
      "93   conv4_block1_out   False\n",
      "94   conv4_block2_1_conv   False\n",
      "95   conv4_block2_1_bn   False\n",
      "96   conv4_block2_1_relu   False\n",
      "97   conv4_block2_2_conv   False\n",
      "98   conv4_block2_2_bn   False\n",
      "99   conv4_block2_2_relu   False\n",
      "100   conv4_block2_3_conv   False\n",
      "101   conv4_block2_3_bn   False\n",
      "102   conv4_block2_add   False\n",
      "103   conv4_block2_out   False\n",
      "104   conv4_block3_1_conv   False\n",
      "105   conv4_block3_1_bn   False\n",
      "106   conv4_block3_1_relu   False\n",
      "107   conv4_block3_2_conv   False\n",
      "108   conv4_block3_2_bn   False\n",
      "109   conv4_block3_2_relu   False\n",
      "110   conv4_block3_3_conv   False\n",
      "111   conv4_block3_3_bn   False\n",
      "112   conv4_block3_add   False\n",
      "113   conv4_block3_out   False\n",
      "114   conv4_block4_1_conv   False\n",
      "115   conv4_block4_1_bn   False\n",
      "116   conv4_block4_1_relu   False\n",
      "117   conv4_block4_2_conv   False\n",
      "118   conv4_block4_2_bn   False\n",
      "119   conv4_block4_2_relu   False\n",
      "120   conv4_block4_3_conv   False\n",
      "121   conv4_block4_3_bn   False\n",
      "122   conv4_block4_add   False\n",
      "123   conv4_block4_out   False\n",
      "124   conv4_block5_1_conv   False\n",
      "125   conv4_block5_1_bn   False\n",
      "126   conv4_block5_1_relu   False\n",
      "127   conv4_block5_2_conv   False\n",
      "128   conv4_block5_2_bn   False\n",
      "129   conv4_block5_2_relu   False\n",
      "130   conv4_block5_3_conv   False\n",
      "131   conv4_block5_3_bn   False\n",
      "132   conv4_block5_add   False\n",
      "133   conv4_block5_out   False\n",
      "134   conv4_block6_1_conv   False\n",
      "135   conv4_block6_1_bn   False\n",
      "136   conv4_block6_1_relu   False\n",
      "137   conv4_block6_2_conv   False\n",
      "138   conv4_block6_2_bn   False\n",
      "139   conv4_block6_2_relu   False\n",
      "140   conv4_block6_3_conv   False\n",
      "141   conv4_block6_3_bn   False\n",
      "142   conv4_block6_add   False\n",
      "143   conv4_block6_out   False\n",
      "144   conv5_block1_1_conv   False\n",
      "145   conv5_block1_1_bn   False\n",
      "146   conv5_block1_1_relu   False\n",
      "147   conv5_block1_2_conv   False\n",
      "148   conv5_block1_2_bn   False\n",
      "149   conv5_block1_2_relu   False\n",
      "150   conv5_block1_0_conv   False\n",
      "151   conv5_block1_3_conv   False\n",
      "152   conv5_block1_0_bn   False\n",
      "153   conv5_block1_3_bn   False\n",
      "154   conv5_block1_add   False\n",
      "155   conv5_block1_out   False\n",
      "156   conv5_block2_1_conv   False\n",
      "157   conv5_block2_1_bn   False\n",
      "158   conv5_block2_1_relu   False\n",
      "159   conv5_block2_2_conv   False\n",
      "160   conv5_block2_2_bn   False\n",
      "161   conv5_block2_2_relu   False\n",
      "162   conv5_block2_3_conv   False\n",
      "163   conv5_block2_3_bn   False\n",
      "164   conv5_block2_add   False\n",
      "165   conv5_block2_out   False\n",
      "166   conv5_block3_1_conv   False\n",
      "167   conv5_block3_1_bn   False\n",
      "168   conv5_block3_1_relu   False\n",
      "169   conv5_block3_2_conv   False\n",
      "170   conv5_block3_2_bn   False\n",
      "171   conv5_block3_2_relu   False\n",
      "172   conv5_block3_3_conv   False\n",
      "173   conv5_block3_3_bn   False\n",
      "174   conv5_block3_add   False\n",
      "175   conv5_block3_out   True\n",
      "model retrieved\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Functional)        (None, 7, 7, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 100352)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              102761472 \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 128,458,634\n",
      "Trainable params: 104,870,922\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n",
      "model building done\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 449s 6s/step - loss: 9.2507 - accuracy: 0.1060 - val_loss: 2.2614 - val_accuracy: 0.1469\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 452s 6s/step - loss: 2.2743 - accuracy: 0.1527 - val_loss: 2.2605 - val_accuracy: 0.1375\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 466s 6s/step - loss: 2.2808 - accuracy: 0.1291 - val_loss: 2.2749 - val_accuracy: 0.1469\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 426s 5s/step - loss: 2.3012 - accuracy: 0.1243 - val_loss: 2.2339 - val_accuracy: 0.1469\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 433s 5s/step - loss: 2.2828 - accuracy: 0.1320 - val_loss: 2.3036 - val_accuracy: 0.0938\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 442s 6s/step - loss: 2.3056 - accuracy: 0.0945 - val_loss: 2.3007 - val_accuracy: 0.0953\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 423s 5s/step - loss: 2.3044 - accuracy: 0.1020 - val_loss: 2.3032 - val_accuracy: 0.0969\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 424s 5s/step - loss: 2.3038 - accuracy: 0.0975 - val_loss: 2.3031 - val_accuracy: 0.0953\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 423s 5s/step - loss: 2.3035 - accuracy: 0.0957 - val_loss: 2.3025 - val_accuracy: 0.1016\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 420s 5s/step - loss: 2.3027 - accuracy: 0.1013 - val_loss: 2.3026 - val_accuracy: 0.0969\n",
      "32/32 [==============================] - 136s 4s/step - loss: 2.3026 - accuracy: 0.1000\n",
      "model training done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 23887<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210412_215813-loj2oxgh/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210412_215813-loj2oxgh/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>2.30293</td></tr><tr><td>accuracy</td><td>0.09924</td></tr><tr><td>val_loss</td><td>2.30255</td></tr><tr><td>val_accuracy</td><td>0.09687</td></tr><tr><td>_runtime</td><td>4503</td></tr><tr><td>_timestamp</td><td>1618249396</td></tr><tr><td>_step</td><td>10</td></tr><tr><td>best_val_accuracy</td><td>0.14687</td></tr><tr><td>best_epoch</td><td>0</td></tr><tr><td>test_acc </td><td>0.1</td></tr><tr><td>test_loss </td><td>2.30257</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>accuracy</td><td>▄█▆▅▄▂▂▂▁▂</td></tr><tr><td>val_loss</td><td>▄▄▅▁██████</td></tr><tr><td>val_accuracy</td><td>█▇██▁▁▁▁▂▁</td></tr><tr><td>_runtime</td><td>▁▂▃▃▄▅▆▆▇██</td></tr><tr><td>_timestamp</td><td>▁▂▃▃▄▅▆▆▇██</td></tr><tr><td>_step</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>test_acc </td><td>▁</td></tr><tr><td>test_loss </td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">cs6910-a2-partB</strong>: <a href=\"https://wandb.ai/notarchana/cs6910-a2/runs/loj2oxgh\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/runs/loj2oxgh</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: m607hl1d with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: ['ResNet50', 165, 1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.25<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">cs6910-a2-partB</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/notarchana/cs6910-a2\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/notarchana/cs6910-a2/sweeps/wyy702q1\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/sweeps/wyy702q1</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/notarchana/cs6910-a2/runs/m607hl1d\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/runs/m607hl1d</a><br/>\n",
       "                Run data is saved locally in <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210412_231505-m607hl1d</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "1   input_1   False\n",
      "2   conv1_pad   False\n",
      "3   conv1_conv   False\n",
      "4   conv1_bn   False\n",
      "5   conv1_relu   False\n",
      "6   pool1_pad   False\n",
      "7   pool1_pool   False\n",
      "8   conv2_block1_1_conv   False\n",
      "9   conv2_block1_1_bn   False\n",
      "10   conv2_block1_1_relu   False\n",
      "11   conv2_block1_2_conv   False\n",
      "12   conv2_block1_2_bn   False\n",
      "13   conv2_block1_2_relu   False\n",
      "14   conv2_block1_0_conv   False\n",
      "15   conv2_block1_3_conv   False\n",
      "16   conv2_block1_0_bn   False\n",
      "17   conv2_block1_3_bn   False\n",
      "18   conv2_block1_add   False\n",
      "19   conv2_block1_out   False\n",
      "20   conv2_block2_1_conv   False\n",
      "21   conv2_block2_1_bn   False\n",
      "22   conv2_block2_1_relu   False\n",
      "23   conv2_block2_2_conv   False\n",
      "24   conv2_block2_2_bn   False\n",
      "25   conv2_block2_2_relu   False\n",
      "26   conv2_block2_3_conv   False\n",
      "27   conv2_block2_3_bn   False\n",
      "28   conv2_block2_add   False\n",
      "29   conv2_block2_out   False\n",
      "30   conv2_block3_1_conv   False\n",
      "31   conv2_block3_1_bn   False\n",
      "32   conv2_block3_1_relu   False\n",
      "33   conv2_block3_2_conv   False\n",
      "34   conv2_block3_2_bn   False\n",
      "35   conv2_block3_2_relu   False\n",
      "36   conv2_block3_3_conv   False\n",
      "37   conv2_block3_3_bn   False\n",
      "38   conv2_block3_add   False\n",
      "39   conv2_block3_out   False\n",
      "40   conv3_block1_1_conv   False\n",
      "41   conv3_block1_1_bn   False\n",
      "42   conv3_block1_1_relu   False\n",
      "43   conv3_block1_2_conv   False\n",
      "44   conv3_block1_2_bn   False\n",
      "45   conv3_block1_2_relu   False\n",
      "46   conv3_block1_0_conv   False\n",
      "47   conv3_block1_3_conv   False\n",
      "48   conv3_block1_0_bn   False\n",
      "49   conv3_block1_3_bn   False\n",
      "50   conv3_block1_add   False\n",
      "51   conv3_block1_out   False\n",
      "52   conv3_block2_1_conv   False\n",
      "53   conv3_block2_1_bn   False\n",
      "54   conv3_block2_1_relu   False\n",
      "55   conv3_block2_2_conv   False\n",
      "56   conv3_block2_2_bn   False\n",
      "57   conv3_block2_2_relu   False\n",
      "58   conv3_block2_3_conv   False\n",
      "59   conv3_block2_3_bn   False\n",
      "60   conv3_block2_add   False\n",
      "61   conv3_block2_out   False\n",
      "62   conv3_block3_1_conv   False\n",
      "63   conv3_block3_1_bn   False\n",
      "64   conv3_block3_1_relu   False\n",
      "65   conv3_block3_2_conv   False\n",
      "66   conv3_block3_2_bn   False\n",
      "67   conv3_block3_2_relu   False\n",
      "68   conv3_block3_3_conv   False\n",
      "69   conv3_block3_3_bn   False\n",
      "70   conv3_block3_add   False\n",
      "71   conv3_block3_out   False\n",
      "72   conv3_block4_1_conv   False\n",
      "73   conv3_block4_1_bn   False\n",
      "74   conv3_block4_1_relu   False\n",
      "75   conv3_block4_2_conv   False\n",
      "76   conv3_block4_2_bn   False\n",
      "77   conv3_block4_2_relu   False\n",
      "78   conv3_block4_3_conv   False\n",
      "79   conv3_block4_3_bn   False\n",
      "80   conv3_block4_add   False\n",
      "81   conv3_block4_out   False\n",
      "82   conv4_block1_1_conv   False\n",
      "83   conv4_block1_1_bn   False\n",
      "84   conv4_block1_1_relu   False\n",
      "85   conv4_block1_2_conv   False\n",
      "86   conv4_block1_2_bn   False\n",
      "87   conv4_block1_2_relu   False\n",
      "88   conv4_block1_0_conv   False\n",
      "89   conv4_block1_3_conv   False\n",
      "90   conv4_block1_0_bn   False\n",
      "91   conv4_block1_3_bn   False\n",
      "92   conv4_block1_add   False\n",
      "93   conv4_block1_out   False\n",
      "94   conv4_block2_1_conv   False\n",
      "95   conv4_block2_1_bn   False\n",
      "96   conv4_block2_1_relu   False\n",
      "97   conv4_block2_2_conv   False\n",
      "98   conv4_block2_2_bn   False\n",
      "99   conv4_block2_2_relu   False\n",
      "100   conv4_block2_3_conv   False\n",
      "101   conv4_block2_3_bn   False\n",
      "102   conv4_block2_add   False\n",
      "103   conv4_block2_out   False\n",
      "104   conv4_block3_1_conv   False\n",
      "105   conv4_block3_1_bn   False\n",
      "106   conv4_block3_1_relu   False\n",
      "107   conv4_block3_2_conv   False\n",
      "108   conv4_block3_2_bn   False\n",
      "109   conv4_block3_2_relu   False\n",
      "110   conv4_block3_3_conv   False\n",
      "111   conv4_block3_3_bn   False\n",
      "112   conv4_block3_add   False\n",
      "113   conv4_block3_out   False\n",
      "114   conv4_block4_1_conv   False\n",
      "115   conv4_block4_1_bn   False\n",
      "116   conv4_block4_1_relu   False\n",
      "117   conv4_block4_2_conv   False\n",
      "118   conv4_block4_2_bn   False\n",
      "119   conv4_block4_2_relu   False\n",
      "120   conv4_block4_3_conv   False\n",
      "121   conv4_block4_3_bn   False\n",
      "122   conv4_block4_add   False\n",
      "123   conv4_block4_out   False\n",
      "124   conv4_block5_1_conv   False\n",
      "125   conv4_block5_1_bn   False\n",
      "126   conv4_block5_1_relu   False\n",
      "127   conv4_block5_2_conv   False\n",
      "128   conv4_block5_2_bn   False\n",
      "129   conv4_block5_2_relu   False\n",
      "130   conv4_block5_3_conv   False\n",
      "131   conv4_block5_3_bn   False\n",
      "132   conv4_block5_add   False\n",
      "133   conv4_block5_out   False\n",
      "134   conv4_block6_1_conv   False\n",
      "135   conv4_block6_1_bn   False\n",
      "136   conv4_block6_1_relu   False\n",
      "137   conv4_block6_2_conv   False\n",
      "138   conv4_block6_2_bn   False\n",
      "139   conv4_block6_2_relu   False\n",
      "140   conv4_block6_3_conv   False\n",
      "141   conv4_block6_3_bn   False\n",
      "142   conv4_block6_add   False\n",
      "143   conv4_block6_out   False\n",
      "144   conv5_block1_1_conv   False\n",
      "145   conv5_block1_1_bn   False\n",
      "146   conv5_block1_1_relu   False\n",
      "147   conv5_block1_2_conv   False\n",
      "148   conv5_block1_2_bn   False\n",
      "149   conv5_block1_2_relu   False\n",
      "150   conv5_block1_0_conv   False\n",
      "151   conv5_block1_3_conv   False\n",
      "152   conv5_block1_0_bn   False\n",
      "153   conv5_block1_3_bn   False\n",
      "154   conv5_block1_add   False\n",
      "155   conv5_block1_out   False\n",
      "156   conv5_block2_1_conv   False\n",
      "157   conv5_block2_1_bn   False\n",
      "158   conv5_block2_1_relu   False\n",
      "159   conv5_block2_2_conv   False\n",
      "160   conv5_block2_2_bn   False\n",
      "161   conv5_block2_2_relu   False\n",
      "162   conv5_block2_3_conv   False\n",
      "163   conv5_block2_3_bn   False\n",
      "164   conv5_block2_add   False\n",
      "165   conv5_block2_out   False\n",
      "166   conv5_block3_1_conv   True\n",
      "167   conv5_block3_1_bn   True\n",
      "168   conv5_block3_1_relu   True\n",
      "169   conv5_block3_2_conv   True\n",
      "170   conv5_block3_2_bn   True\n",
      "171   conv5_block3_2_relu   True\n",
      "172   conv5_block3_3_conv   True\n",
      "173   conv5_block3_3_bn   True\n",
      "174   conv5_block3_add   True\n",
      "175   conv5_block3_out   True\n",
      "model retrieved\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Functional)        (None, 7, 7, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 100352)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              102761472 \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 128,458,634\n",
      "Trainable params: 109,336,586\n",
      "Non-trainable params: 19,122,048\n",
      "_________________________________________________________________\n",
      "model building done\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 468s 6s/step - loss: 7.0720 - accuracy: 0.0961 - val_loss: 2.3847 - val_accuracy: 0.0984\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 503s 6s/step - loss: 2.2757 - accuracy: 0.1492 - val_loss: 2.2911 - val_accuracy: 0.1266\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 535s 7s/step - loss: 2.2606 - accuracy: 0.1554 - val_loss: 2.2171 - val_accuracy: 0.1562\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 485s 6s/step - loss: 2.2582 - accuracy: 0.1542 - val_loss: 2.5023 - val_accuracy: 0.0797\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 464s 6s/step - loss: 2.2312 - accuracy: 0.1603 - val_loss: 2.5244 - val_accuracy: 0.1219\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 497s 6s/step - loss: 2.2307 - accuracy: 0.1682 - val_loss: 2.3437 - val_accuracy: 0.1375\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 499s 6s/step - loss: 2.2178 - accuracy: 0.1648 - val_loss: 2.5873 - val_accuracy: 0.1234\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 480s 6s/step - loss: 2.1915 - accuracy: 0.1818 - val_loss: 2.2375 - val_accuracy: 0.1734\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 463s 6s/step - loss: 2.1868 - accuracy: 0.1914 - val_loss: 2.3555 - val_accuracy: 0.1328\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 464s 6s/step - loss: 2.1683 - accuracy: 0.1975 - val_loss: 2.3037 - val_accuracy: 0.1531\n",
      "32/32 [==============================] - 128s 4s/step - loss: 2.3032 - accuracy: 0.1460\n",
      "model training done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 24743<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210412_231505-m607hl1d/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210412_231505-m607hl1d/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>2.16906</td></tr><tr><td>accuracy</td><td>0.20195</td></tr><tr><td>val_loss</td><td>2.3037</td></tr><tr><td>val_accuracy</td><td>0.15313</td></tr><tr><td>_runtime</td><td>5003</td></tr><tr><td>_timestamp</td><td>1618254508</td></tr><tr><td>_step</td><td>10</td></tr><tr><td>best_val_accuracy</td><td>0.17344</td></tr><tr><td>best_epoch</td><td>7</td></tr><tr><td>test_acc </td><td>0.146</td></tr><tr><td>test_loss </td><td>2.3032</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>accuracy</td><td>▁▄▅▅▆▆▆▆▇█</td></tr><tr><td>val_loss</td><td>▄▂▁▆▇▃█▁▄▃</td></tr><tr><td>val_accuracy</td><td>▂▅▇▁▄▅▄█▅▆</td></tr><tr><td>_runtime</td><td>▁▂▃▃▄▅▆▆▇██</td></tr><tr><td>_timestamp</td><td>▁▂▃▃▄▅▆▆▇██</td></tr><tr><td>_step</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>test_acc </td><td>▁</td></tr><tr><td>test_loss </td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">cs6910-a2-partB</strong>: <a href=\"https://wandb.ai/notarchana/cs6910-a2/runs/m607hl1d\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/runs/m607hl1d</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8hyc33tn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: ['ResNet50', 155, 2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.25<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">cs6910-a2-partB</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/notarchana/cs6910-a2\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/notarchana/cs6910-a2/sweeps/wyy702q1\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/sweeps/wyy702q1</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/notarchana/cs6910-a2/runs/8hyc33tn\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/runs/8hyc33tn</a><br/>\n",
       "                Run data is saved locally in <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210413_004115-8hyc33tn</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\n",
      "1   input_1   False\n",
      "2   conv1_pad   False\n",
      "3   conv1_conv   False\n",
      "4   conv1_bn   False\n",
      "5   conv1_relu   False\n",
      "6   pool1_pad   False\n",
      "7   pool1_pool   False\n",
      "8   conv2_block1_1_conv   False\n",
      "9   conv2_block1_1_bn   False\n",
      "10   conv2_block1_1_relu   False\n",
      "11   conv2_block1_2_conv   False\n",
      "12   conv2_block1_2_bn   False\n",
      "13   conv2_block1_2_relu   False\n",
      "14   conv2_block1_0_conv   False\n",
      "15   conv2_block1_3_conv   False\n",
      "16   conv2_block1_0_bn   False\n",
      "17   conv2_block1_3_bn   False\n",
      "18   conv2_block1_add   False\n",
      "19   conv2_block1_out   False\n",
      "20   conv2_block2_1_conv   False\n",
      "21   conv2_block2_1_bn   False\n",
      "22   conv2_block2_1_relu   False\n",
      "23   conv2_block2_2_conv   False\n",
      "24   conv2_block2_2_bn   False\n",
      "25   conv2_block2_2_relu   False\n",
      "26   conv2_block2_3_conv   False\n",
      "27   conv2_block2_3_bn   False\n",
      "28   conv2_block2_add   False\n",
      "29   conv2_block2_out   False\n",
      "30   conv2_block3_1_conv   False\n",
      "31   conv2_block3_1_bn   False\n",
      "32   conv2_block3_1_relu   False\n",
      "33   conv2_block3_2_conv   False\n",
      "34   conv2_block3_2_bn   False\n",
      "35   conv2_block3_2_relu   False\n",
      "36   conv2_block3_3_conv   False\n",
      "37   conv2_block3_3_bn   False\n",
      "38   conv2_block3_add   False\n",
      "39   conv2_block3_out   False\n",
      "40   conv3_block1_1_conv   False\n",
      "41   conv3_block1_1_bn   False\n",
      "42   conv3_block1_1_relu   False\n",
      "43   conv3_block1_2_conv   False\n",
      "44   conv3_block1_2_bn   False\n",
      "45   conv3_block1_2_relu   False\n",
      "46   conv3_block1_0_conv   False\n",
      "47   conv3_block1_3_conv   False\n",
      "48   conv3_block1_0_bn   False\n",
      "49   conv3_block1_3_bn   False\n",
      "50   conv3_block1_add   False\n",
      "51   conv3_block1_out   False\n",
      "52   conv3_block2_1_conv   False\n",
      "53   conv3_block2_1_bn   False\n",
      "54   conv3_block2_1_relu   False\n",
      "55   conv3_block2_2_conv   False\n",
      "56   conv3_block2_2_bn   False\n",
      "57   conv3_block2_2_relu   False\n",
      "58   conv3_block2_3_conv   False\n",
      "59   conv3_block2_3_bn   False\n",
      "60   conv3_block2_add   False\n",
      "61   conv3_block2_out   False\n",
      "62   conv3_block3_1_conv   False\n",
      "63   conv3_block3_1_bn   False\n",
      "64   conv3_block3_1_relu   False\n",
      "65   conv3_block3_2_conv   False\n",
      "66   conv3_block3_2_bn   False\n",
      "67   conv3_block3_2_relu   False\n",
      "68   conv3_block3_3_conv   False\n",
      "69   conv3_block3_3_bn   False\n",
      "70   conv3_block3_add   False\n",
      "71   conv3_block3_out   False\n",
      "72   conv3_block4_1_conv   False\n",
      "73   conv3_block4_1_bn   False\n",
      "74   conv3_block4_1_relu   False\n",
      "75   conv3_block4_2_conv   False\n",
      "76   conv3_block4_2_bn   False\n",
      "77   conv3_block4_2_relu   False\n",
      "78   conv3_block4_3_conv   False\n",
      "79   conv3_block4_3_bn   False\n",
      "80   conv3_block4_add   False\n",
      "81   conv3_block4_out   False\n",
      "82   conv4_block1_1_conv   False\n",
      "83   conv4_block1_1_bn   False\n",
      "84   conv4_block1_1_relu   False\n",
      "85   conv4_block1_2_conv   False\n",
      "86   conv4_block1_2_bn   False\n",
      "87   conv4_block1_2_relu   False\n",
      "88   conv4_block1_0_conv   False\n",
      "89   conv4_block1_3_conv   False\n",
      "90   conv4_block1_0_bn   False\n",
      "91   conv4_block1_3_bn   False\n",
      "92   conv4_block1_add   False\n",
      "93   conv4_block1_out   False\n",
      "94   conv4_block2_1_conv   False\n",
      "95   conv4_block2_1_bn   False\n",
      "96   conv4_block2_1_relu   False\n",
      "97   conv4_block2_2_conv   False\n",
      "98   conv4_block2_2_bn   False\n",
      "99   conv4_block2_2_relu   False\n",
      "100   conv4_block2_3_conv   False\n",
      "101   conv4_block2_3_bn   False\n",
      "102   conv4_block2_add   False\n",
      "103   conv4_block2_out   False\n",
      "104   conv4_block3_1_conv   False\n",
      "105   conv4_block3_1_bn   False\n",
      "106   conv4_block3_1_relu   False\n",
      "107   conv4_block3_2_conv   False\n",
      "108   conv4_block3_2_bn   False\n",
      "109   conv4_block3_2_relu   False\n",
      "110   conv4_block3_3_conv   False\n",
      "111   conv4_block3_3_bn   False\n",
      "112   conv4_block3_add   False\n",
      "113   conv4_block3_out   False\n",
      "114   conv4_block4_1_conv   False\n",
      "115   conv4_block4_1_bn   False\n",
      "116   conv4_block4_1_relu   False\n",
      "117   conv4_block4_2_conv   False\n",
      "118   conv4_block4_2_bn   False\n",
      "119   conv4_block4_2_relu   False\n",
      "120   conv4_block4_3_conv   False\n",
      "121   conv4_block4_3_bn   False\n",
      "122   conv4_block4_add   False\n",
      "123   conv4_block4_out   False\n",
      "124   conv4_block5_1_conv   False\n",
      "125   conv4_block5_1_bn   False\n",
      "126   conv4_block5_1_relu   False\n",
      "127   conv4_block5_2_conv   False\n",
      "128   conv4_block5_2_bn   False\n",
      "129   conv4_block5_2_relu   False\n",
      "130   conv4_block5_3_conv   False\n",
      "131   conv4_block5_3_bn   False\n",
      "132   conv4_block5_add   False\n",
      "133   conv4_block5_out   False\n",
      "134   conv4_block6_1_conv   False\n",
      "135   conv4_block6_1_bn   False\n",
      "136   conv4_block6_1_relu   False\n",
      "137   conv4_block6_2_conv   False\n",
      "138   conv4_block6_2_bn   False\n",
      "139   conv4_block6_2_relu   False\n",
      "140   conv4_block6_3_conv   False\n",
      "141   conv4_block6_3_bn   False\n",
      "142   conv4_block6_add   False\n",
      "143   conv4_block6_out   False\n",
      "144   conv5_block1_1_conv   False\n",
      "145   conv5_block1_1_bn   False\n",
      "146   conv5_block1_1_relu   False\n",
      "147   conv5_block1_2_conv   False\n",
      "148   conv5_block1_2_bn   False\n",
      "149   conv5_block1_2_relu   False\n",
      "150   conv5_block1_0_conv   False\n",
      "151   conv5_block1_3_conv   False\n",
      "152   conv5_block1_0_bn   False\n",
      "153   conv5_block1_3_bn   False\n",
      "154   conv5_block1_add   False\n",
      "155   conv5_block1_out   False\n",
      "156   conv5_block2_1_conv   True\n",
      "157   conv5_block2_1_bn   True\n",
      "158   conv5_block2_1_relu   True\n",
      "159   conv5_block2_2_conv   True\n",
      "160   conv5_block2_2_bn   True\n",
      "161   conv5_block2_2_relu   True\n",
      "162   conv5_block2_3_conv   True\n",
      "163   conv5_block2_3_bn   True\n",
      "164   conv5_block2_add   True\n",
      "165   conv5_block2_out   True\n",
      "166   conv5_block3_1_conv   True\n",
      "167   conv5_block3_1_bn   True\n",
      "168   conv5_block3_1_relu   True\n",
      "169   conv5_block3_2_conv   True\n",
      "170   conv5_block3_2_bn   True\n",
      "171   conv5_block3_2_relu   True\n",
      "172   conv5_block3_3_conv   True\n",
      "173   conv5_block3_3_bn   True\n",
      "174   conv5_block3_add   True\n",
      "175   conv5_block3_out   True\n",
      "model retrieved\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Functional)        (None, 7, 7, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 100352)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              102761472 \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 128,458,634\n",
      "Trainable params: 113,802,250\n",
      "Non-trainable params: 14,656,384\n",
      "_________________________________________________________________\n",
      "model building done\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 492s 6s/step - loss: 6.7258 - accuracy: 0.1105 - val_loss: 2.9195 - val_accuracy: 0.1000\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 488s 6s/step - loss: 2.2862 - accuracy: 0.1229 - val_loss: 2.3027 - val_accuracy: 0.0969\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 491s 6s/step - loss: 2.3023 - accuracy: 0.1098 - val_loss: 2.3027 - val_accuracy: 0.1031\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 547s 7s/step - loss: 2.3030 - accuracy: 0.0984 - val_loss: 2.3026 - val_accuracy: 0.1031\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 516s 6s/step - loss: 2.3027 - accuracy: 0.0947 - val_loss: 2.3016 - val_accuracy: 0.0953\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 552s 7s/step - loss: 2.3017 - accuracy: 0.1029 - val_loss: 2.3089 - val_accuracy: 0.1109\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 518s 6s/step - loss: 2.2994 - accuracy: 0.1066 - val_loss: 2.3021 - val_accuracy: 0.1094\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 517s 6s/step - loss: 2.3022 - accuracy: 0.1078 - val_loss: 2.3030 - val_accuracy: 0.0859\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 511s 6s/step - loss: 2.3027 - accuracy: 0.0968 - val_loss: 2.3030 - val_accuracy: 0.0922\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 512s 6s/step - loss: 2.3028 - accuracy: 0.0987 - val_loss: 2.3027 - val_accuracy: 0.1031\n",
      "32/32 [==============================] - 133s 4s/step - loss: 2.3027 - accuracy: 0.1000\n",
      "model training done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 26325<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210413_004115-8hyc33tn/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210413_004115-8hyc33tn/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>2.30273</td></tr><tr><td>accuracy</td><td>0.09727</td></tr><tr><td>val_loss</td><td>2.30271</td></tr><tr><td>val_accuracy</td><td>0.10312</td></tr><tr><td>_runtime</td><td>5291</td></tr><tr><td>_timestamp</td><td>1618259966</td></tr><tr><td>_step</td><td>10</td></tr><tr><td>best_val_accuracy</td><td>0.11094</td></tr><tr><td>best_epoch</td><td>5</td></tr><tr><td>test_acc </td><td>0.1</td></tr><tr><td>test_loss </td><td>2.30269</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>accuracy</td><td>█▆▃▁▂▂▃▃▁▁</td></tr><tr><td>val_loss</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▅▄▆▆▄██▁▃▆</td></tr><tr><td>_runtime</td><td>▁▂▂▃▄▅▆▆▇██</td></tr><tr><td>_timestamp</td><td>▁▂▂▃▄▅▆▆▇██</td></tr><tr><td>_step</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>test_acc </td><td>▁</td></tr><tr><td>test_loss </td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">cs6910-a2-partB</strong>: <a href=\"https://wandb.ai/notarchana/cs6910-a2/runs/8hyc33tn\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/runs/8hyc33tn</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a3isu13q with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: ['InceptionV3', 310, 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.25<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">cs6910-a2-partB</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/notarchana/cs6910-a2\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/notarchana/cs6910-a2/sweeps/wyy702q1\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/sweeps/wyy702q1</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/notarchana/cs6910-a2/runs/a3isu13q\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/runs/a3isu13q</a><br/>\n",
       "                Run data is saved locally in <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210413_021215-a3isu13q</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310\n",
      "1   input_1   False\n",
      "2   conv2d   False\n",
      "3   batch_normalization   False\n",
      "4   activation   False\n",
      "5   conv2d_1   False\n",
      "6   batch_normalization_1   False\n",
      "7   activation_1   False\n",
      "8   conv2d_2   False\n",
      "9   batch_normalization_2   False\n",
      "10   activation_2   False\n",
      "11   max_pooling2d   False\n",
      "12   conv2d_3   False\n",
      "13   batch_normalization_3   False\n",
      "14   activation_3   False\n",
      "15   conv2d_4   False\n",
      "16   batch_normalization_4   False\n",
      "17   activation_4   False\n",
      "18   max_pooling2d_1   False\n",
      "19   conv2d_8   False\n",
      "20   batch_normalization_8   False\n",
      "21   activation_8   False\n",
      "22   conv2d_6   False\n",
      "23   conv2d_9   False\n",
      "24   batch_normalization_6   False\n",
      "25   batch_normalization_9   False\n",
      "26   activation_6   False\n",
      "27   activation_9   False\n",
      "28   average_pooling2d   False\n",
      "29   conv2d_5   False\n",
      "30   conv2d_7   False\n",
      "31   conv2d_10   False\n",
      "32   conv2d_11   False\n",
      "33   batch_normalization_5   False\n",
      "34   batch_normalization_7   False\n",
      "35   batch_normalization_10   False\n",
      "36   batch_normalization_11   False\n",
      "37   activation_5   False\n",
      "38   activation_7   False\n",
      "39   activation_10   False\n",
      "40   activation_11   False\n",
      "41   mixed0   False\n",
      "42   conv2d_15   False\n",
      "43   batch_normalization_15   False\n",
      "44   activation_15   False\n",
      "45   conv2d_13   False\n",
      "46   conv2d_16   False\n",
      "47   batch_normalization_13   False\n",
      "48   batch_normalization_16   False\n",
      "49   activation_13   False\n",
      "50   activation_16   False\n",
      "51   average_pooling2d_1   False\n",
      "52   conv2d_12   False\n",
      "53   conv2d_14   False\n",
      "54   conv2d_17   False\n",
      "55   conv2d_18   False\n",
      "56   batch_normalization_12   False\n",
      "57   batch_normalization_14   False\n",
      "58   batch_normalization_17   False\n",
      "59   batch_normalization_18   False\n",
      "60   activation_12   False\n",
      "61   activation_14   False\n",
      "62   activation_17   False\n",
      "63   activation_18   False\n",
      "64   mixed1   False\n",
      "65   conv2d_22   False\n",
      "66   batch_normalization_22   False\n",
      "67   activation_22   False\n",
      "68   conv2d_20   False\n",
      "69   conv2d_23   False\n",
      "70   batch_normalization_20   False\n",
      "71   batch_normalization_23   False\n",
      "72   activation_20   False\n",
      "73   activation_23   False\n",
      "74   average_pooling2d_2   False\n",
      "75   conv2d_19   False\n",
      "76   conv2d_21   False\n",
      "77   conv2d_24   False\n",
      "78   conv2d_25   False\n",
      "79   batch_normalization_19   False\n",
      "80   batch_normalization_21   False\n",
      "81   batch_normalization_24   False\n",
      "82   batch_normalization_25   False\n",
      "83   activation_19   False\n",
      "84   activation_21   False\n",
      "85   activation_24   False\n",
      "86   activation_25   False\n",
      "87   mixed2   False\n",
      "88   conv2d_27   False\n",
      "89   batch_normalization_27   False\n",
      "90   activation_27   False\n",
      "91   conv2d_28   False\n",
      "92   batch_normalization_28   False\n",
      "93   activation_28   False\n",
      "94   conv2d_26   False\n",
      "95   conv2d_29   False\n",
      "96   batch_normalization_26   False\n",
      "97   batch_normalization_29   False\n",
      "98   activation_26   False\n",
      "99   activation_29   False\n",
      "100   max_pooling2d_2   False\n",
      "101   mixed3   False\n",
      "102   conv2d_34   False\n",
      "103   batch_normalization_34   False\n",
      "104   activation_34   False\n",
      "105   conv2d_35   False\n",
      "106   batch_normalization_35   False\n",
      "107   activation_35   False\n",
      "108   conv2d_31   False\n",
      "109   conv2d_36   False\n",
      "110   batch_normalization_31   False\n",
      "111   batch_normalization_36   False\n",
      "112   activation_31   False\n",
      "113   activation_36   False\n",
      "114   conv2d_32   False\n",
      "115   conv2d_37   False\n",
      "116   batch_normalization_32   False\n",
      "117   batch_normalization_37   False\n",
      "118   activation_32   False\n",
      "119   activation_37   False\n",
      "120   average_pooling2d_3   False\n",
      "121   conv2d_30   False\n",
      "122   conv2d_33   False\n",
      "123   conv2d_38   False\n",
      "124   conv2d_39   False\n",
      "125   batch_normalization_30   False\n",
      "126   batch_normalization_33   False\n",
      "127   batch_normalization_38   False\n",
      "128   batch_normalization_39   False\n",
      "129   activation_30   False\n",
      "130   activation_33   False\n",
      "131   activation_38   False\n",
      "132   activation_39   False\n",
      "133   mixed4   False\n",
      "134   conv2d_44   False\n",
      "135   batch_normalization_44   False\n",
      "136   activation_44   False\n",
      "137   conv2d_45   False\n",
      "138   batch_normalization_45   False\n",
      "139   activation_45   False\n",
      "140   conv2d_41   False\n",
      "141   conv2d_46   False\n",
      "142   batch_normalization_41   False\n",
      "143   batch_normalization_46   False\n",
      "144   activation_41   False\n",
      "145   activation_46   False\n",
      "146   conv2d_42   False\n",
      "147   conv2d_47   False\n",
      "148   batch_normalization_42   False\n",
      "149   batch_normalization_47   False\n",
      "150   activation_42   False\n",
      "151   activation_47   False\n",
      "152   average_pooling2d_4   False\n",
      "153   conv2d_40   False\n",
      "154   conv2d_43   False\n",
      "155   conv2d_48   False\n",
      "156   conv2d_49   False\n",
      "157   batch_normalization_40   False\n",
      "158   batch_normalization_43   False\n",
      "159   batch_normalization_48   False\n",
      "160   batch_normalization_49   False\n",
      "161   activation_40   False\n",
      "162   activation_43   False\n",
      "163   activation_48   False\n",
      "164   activation_49   False\n",
      "165   mixed5   False\n",
      "166   conv2d_54   False\n",
      "167   batch_normalization_54   False\n",
      "168   activation_54   False\n",
      "169   conv2d_55   False\n",
      "170   batch_normalization_55   False\n",
      "171   activation_55   False\n",
      "172   conv2d_51   False\n",
      "173   conv2d_56   False\n",
      "174   batch_normalization_51   False\n",
      "175   batch_normalization_56   False\n",
      "176   activation_51   False\n",
      "177   activation_56   False\n",
      "178   conv2d_52   False\n",
      "179   conv2d_57   False\n",
      "180   batch_normalization_52   False\n",
      "181   batch_normalization_57   False\n",
      "182   activation_52   False\n",
      "183   activation_57   False\n",
      "184   average_pooling2d_5   False\n",
      "185   conv2d_50   False\n",
      "186   conv2d_53   False\n",
      "187   conv2d_58   False\n",
      "188   conv2d_59   False\n",
      "189   batch_normalization_50   False\n",
      "190   batch_normalization_53   False\n",
      "191   batch_normalization_58   False\n",
      "192   batch_normalization_59   False\n",
      "193   activation_50   False\n",
      "194   activation_53   False\n",
      "195   activation_58   False\n",
      "196   activation_59   False\n",
      "197   mixed6   False\n",
      "198   conv2d_64   False\n",
      "199   batch_normalization_64   False\n",
      "200   activation_64   False\n",
      "201   conv2d_65   False\n",
      "202   batch_normalization_65   False\n",
      "203   activation_65   False\n",
      "204   conv2d_61   False\n",
      "205   conv2d_66   False\n",
      "206   batch_normalization_61   False\n",
      "207   batch_normalization_66   False\n",
      "208   activation_61   False\n",
      "209   activation_66   False\n",
      "210   conv2d_62   False\n",
      "211   conv2d_67   False\n",
      "212   batch_normalization_62   False\n",
      "213   batch_normalization_67   False\n",
      "214   activation_62   False\n",
      "215   activation_67   False\n",
      "216   average_pooling2d_6   False\n",
      "217   conv2d_60   False\n",
      "218   conv2d_63   False\n",
      "219   conv2d_68   False\n",
      "220   conv2d_69   False\n",
      "221   batch_normalization_60   False\n",
      "222   batch_normalization_63   False\n",
      "223   batch_normalization_68   False\n",
      "224   batch_normalization_69   False\n",
      "225   activation_60   False\n",
      "226   activation_63   False\n",
      "227   activation_68   False\n",
      "228   activation_69   False\n",
      "229   mixed7   False\n",
      "230   conv2d_72   False\n",
      "231   batch_normalization_72   False\n",
      "232   activation_72   False\n",
      "233   conv2d_73   False\n",
      "234   batch_normalization_73   False\n",
      "235   activation_73   False\n",
      "236   conv2d_70   False\n",
      "237   conv2d_74   False\n",
      "238   batch_normalization_70   False\n",
      "239   batch_normalization_74   False\n",
      "240   activation_70   False\n",
      "241   activation_74   False\n",
      "242   conv2d_71   False\n",
      "243   conv2d_75   False\n",
      "244   batch_normalization_71   False\n",
      "245   batch_normalization_75   False\n",
      "246   activation_71   False\n",
      "247   activation_75   False\n",
      "248   max_pooling2d_3   False\n",
      "249   mixed8   False\n",
      "250   conv2d_80   False\n",
      "251   batch_normalization_80   False\n",
      "252   activation_80   False\n",
      "253   conv2d_77   False\n",
      "254   conv2d_81   False\n",
      "255   batch_normalization_77   False\n",
      "256   batch_normalization_81   False\n",
      "257   activation_77   False\n",
      "258   activation_81   False\n",
      "259   conv2d_78   False\n",
      "260   conv2d_79   False\n",
      "261   conv2d_82   False\n",
      "262   conv2d_83   False\n",
      "263   average_pooling2d_7   False\n",
      "264   conv2d_76   False\n",
      "265   batch_normalization_78   False\n",
      "266   batch_normalization_79   False\n",
      "267   batch_normalization_82   False\n",
      "268   batch_normalization_83   False\n",
      "269   conv2d_84   False\n",
      "270   batch_normalization_76   False\n",
      "271   activation_78   False\n",
      "272   activation_79   False\n",
      "273   activation_82   False\n",
      "274   activation_83   False\n",
      "275   batch_normalization_84   False\n",
      "276   activation_76   False\n",
      "277   mixed9_0   False\n",
      "278   concatenate   False\n",
      "279   activation_84   False\n",
      "280   mixed9   False\n",
      "281   conv2d_89   False\n",
      "282   batch_normalization_89   False\n",
      "283   activation_89   False\n",
      "284   conv2d_86   False\n",
      "285   conv2d_90   False\n",
      "286   batch_normalization_86   False\n",
      "287   batch_normalization_90   False\n",
      "288   activation_86   False\n",
      "289   activation_90   False\n",
      "290   conv2d_87   False\n",
      "291   conv2d_88   False\n",
      "292   conv2d_91   False\n",
      "293   conv2d_92   False\n",
      "294   average_pooling2d_8   False\n",
      "295   conv2d_85   False\n",
      "296   batch_normalization_87   False\n",
      "297   batch_normalization_88   False\n",
      "298   batch_normalization_91   False\n",
      "299   batch_normalization_92   False\n",
      "300   conv2d_93   False\n",
      "301   batch_normalization_85   False\n",
      "302   activation_87   False\n",
      "303   activation_88   False\n",
      "304   activation_91   False\n",
      "305   activation_92   False\n",
      "306   batch_normalization_93   False\n",
      "307   activation_85   False\n",
      "308   mixed9_1   False\n",
      "309   concatenate_1   False\n",
      "310   activation_93   False\n",
      "311   mixed10   True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model retrieved\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Functional)    (None, 5, 5, 2048)        21802784  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 51200)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              52429824  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 76,342,058\n",
      "Trainable params: 54,539,274\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n",
      "model building done\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 332s 4s/step - loss: 18.2296 - accuracy: 0.2962 - val_loss: 1.0421 - val_accuracy: 0.6719\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 327s 4s/step - loss: 0.9464 - accuracy: 0.7001 - val_loss: 0.9966 - val_accuracy: 0.6859\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 326s 4s/step - loss: 0.8428 - accuracy: 0.7233 - val_loss: 0.9710 - val_accuracy: 0.7156\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 361s 5s/step - loss: 0.7866 - accuracy: 0.7406 - val_loss: 0.9296 - val_accuracy: 0.7250\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 337s 4s/step - loss: 0.7507 - accuracy: 0.7573 - val_loss: 0.7962 - val_accuracy: 0.7547\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 330s 4s/step - loss: 0.7307 - accuracy: 0.7581 - val_loss: 0.9552 - val_accuracy: 0.7063\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 323s 4s/step - loss: 0.7008 - accuracy: 0.7736 - val_loss: 0.9743 - val_accuracy: 0.7141\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 322s 4s/step - loss: 0.6385 - accuracy: 0.7975 - val_loss: 0.9192 - val_accuracy: 0.7172\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 317s 4s/step - loss: 0.6487 - accuracy: 0.7785 - val_loss: 0.9676 - val_accuracy: 0.7063\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 336s 4s/step - loss: 0.6139 - accuracy: 0.7983 - val_loss: 0.9378 - val_accuracy: 0.6891\n",
      "32/32 [==============================] - 116s 4s/step - loss: 0.8775 - accuracy: 0.7325\n",
      "model training done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 27306<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210413_021215-a3isu13q/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210413_021215-a3isu13q/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.61006</td></tr><tr><td>accuracy</td><td>0.79486</td></tr><tr><td>val_loss</td><td>0.93783</td></tr><tr><td>val_accuracy</td><td>0.68906</td></tr><tr><td>_runtime</td><td>3441</td></tr><tr><td>_timestamp</td><td>1618263576</td></tr><tr><td>_step</td><td>10</td></tr><tr><td>best_val_accuracy</td><td>0.75469</td></tr><tr><td>best_epoch</td><td>4</td></tr><tr><td>test_acc </td><td>0.7325</td></tr><tr><td>test_loss </td><td>0.87746</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>accuracy</td><td>▁▆▆▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▇▆▅▁▆▆▅▆▅</td></tr><tr><td>val_accuracy</td><td>▁▂▅▅█▄▅▅▄▂</td></tr><tr><td>_runtime</td><td>▁▂▂▃▄▅▆▆▇██</td></tr><tr><td>_timestamp</td><td>▁▂▂▃▄▅▆▆▇██</td></tr><tr><td>_step</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>test_acc </td><td>▁</td></tr><tr><td>test_loss </td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">cs6910-a2-partB</strong>: <a href=\"https://wandb.ai/notarchana/cs6910-a2/runs/a3isu13q\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/runs/a3isu13q</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3bwr0vwf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: ['InceptionV3', 280, 1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.25<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">cs6910-a2-partB</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/notarchana/cs6910-a2\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/notarchana/cs6910-a2/sweeps/wyy702q1\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/sweeps/wyy702q1</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/notarchana/cs6910-a2/runs/3bwr0vwf\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/runs/3bwr0vwf</a><br/>\n",
       "                Run data is saved locally in <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210413_031104-3bwr0vwf</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280\n",
      "1   input_1   False\n",
      "2   conv2d   False\n",
      "3   batch_normalization   False\n",
      "4   activation   False\n",
      "5   conv2d_1   False\n",
      "6   batch_normalization_1   False\n",
      "7   activation_1   False\n",
      "8   conv2d_2   False\n",
      "9   batch_normalization_2   False\n",
      "10   activation_2   False\n",
      "11   max_pooling2d   False\n",
      "12   conv2d_3   False\n",
      "13   batch_normalization_3   False\n",
      "14   activation_3   False\n",
      "15   conv2d_4   False\n",
      "16   batch_normalization_4   False\n",
      "17   activation_4   False\n",
      "18   max_pooling2d_1   False\n",
      "19   conv2d_8   False\n",
      "20   batch_normalization_8   False\n",
      "21   activation_8   False\n",
      "22   conv2d_6   False\n",
      "23   conv2d_9   False\n",
      "24   batch_normalization_6   False\n",
      "25   batch_normalization_9   False\n",
      "26   activation_6   False\n",
      "27   activation_9   False\n",
      "28   average_pooling2d   False\n",
      "29   conv2d_5   False\n",
      "30   conv2d_7   False\n",
      "31   conv2d_10   False\n",
      "32   conv2d_11   False\n",
      "33   batch_normalization_5   False\n",
      "34   batch_normalization_7   False\n",
      "35   batch_normalization_10   False\n",
      "36   batch_normalization_11   False\n",
      "37   activation_5   False\n",
      "38   activation_7   False\n",
      "39   activation_10   False\n",
      "40   activation_11   False\n",
      "41   mixed0   False\n",
      "42   conv2d_15   False\n",
      "43   batch_normalization_15   False\n",
      "44   activation_15   False\n",
      "45   conv2d_13   False\n",
      "46   conv2d_16   False\n",
      "47   batch_normalization_13   False\n",
      "48   batch_normalization_16   False\n",
      "49   activation_13   False\n",
      "50   activation_16   False\n",
      "51   average_pooling2d_1   False\n",
      "52   conv2d_12   False\n",
      "53   conv2d_14   False\n",
      "54   conv2d_17   False\n",
      "55   conv2d_18   False\n",
      "56   batch_normalization_12   False\n",
      "57   batch_normalization_14   False\n",
      "58   batch_normalization_17   False\n",
      "59   batch_normalization_18   False\n",
      "60   activation_12   False\n",
      "61   activation_14   False\n",
      "62   activation_17   False\n",
      "63   activation_18   False\n",
      "64   mixed1   False\n",
      "65   conv2d_22   False\n",
      "66   batch_normalization_22   False\n",
      "67   activation_22   False\n",
      "68   conv2d_20   False\n",
      "69   conv2d_23   False\n",
      "70   batch_normalization_20   False\n",
      "71   batch_normalization_23   False\n",
      "72   activation_20   False\n",
      "73   activation_23   False\n",
      "74   average_pooling2d_2   False\n",
      "75   conv2d_19   False\n",
      "76   conv2d_21   False\n",
      "77   conv2d_24   False\n",
      "78   conv2d_25   False\n",
      "79   batch_normalization_19   False\n",
      "80   batch_normalization_21   False\n",
      "81   batch_normalization_24   False\n",
      "82   batch_normalization_25   False\n",
      "83   activation_19   False\n",
      "84   activation_21   False\n",
      "85   activation_24   False\n",
      "86   activation_25   False\n",
      "87   mixed2   False\n",
      "88   conv2d_27   False\n",
      "89   batch_normalization_27   False\n",
      "90   activation_27   False\n",
      "91   conv2d_28   False\n",
      "92   batch_normalization_28   False\n",
      "93   activation_28   False\n",
      "94   conv2d_26   False\n",
      "95   conv2d_29   False\n",
      "96   batch_normalization_26   False\n",
      "97   batch_normalization_29   False\n",
      "98   activation_26   False\n",
      "99   activation_29   False\n",
      "100   max_pooling2d_2   False\n",
      "101   mixed3   False\n",
      "102   conv2d_34   False\n",
      "103   batch_normalization_34   False\n",
      "104   activation_34   False\n",
      "105   conv2d_35   False\n",
      "106   batch_normalization_35   False\n",
      "107   activation_35   False\n",
      "108   conv2d_31   False\n",
      "109   conv2d_36   False\n",
      "110   batch_normalization_31   False\n",
      "111   batch_normalization_36   False\n",
      "112   activation_31   False\n",
      "113   activation_36   False\n",
      "114   conv2d_32   False\n",
      "115   conv2d_37   False\n",
      "116   batch_normalization_32   False\n",
      "117   batch_normalization_37   False\n",
      "118   activation_32   False\n",
      "119   activation_37   False\n",
      "120   average_pooling2d_3   False\n",
      "121   conv2d_30   False\n",
      "122   conv2d_33   False\n",
      "123   conv2d_38   False\n",
      "124   conv2d_39   False\n",
      "125   batch_normalization_30   False\n",
      "126   batch_normalization_33   False\n",
      "127   batch_normalization_38   False\n",
      "128   batch_normalization_39   False\n",
      "129   activation_30   False\n",
      "130   activation_33   False\n",
      "131   activation_38   False\n",
      "132   activation_39   False\n",
      "133   mixed4   False\n",
      "134   conv2d_44   False\n",
      "135   batch_normalization_44   False\n",
      "136   activation_44   False\n",
      "137   conv2d_45   False\n",
      "138   batch_normalization_45   False\n",
      "139   activation_45   False\n",
      "140   conv2d_41   False\n",
      "141   conv2d_46   False\n",
      "142   batch_normalization_41   False\n",
      "143   batch_normalization_46   False\n",
      "144   activation_41   False\n",
      "145   activation_46   False\n",
      "146   conv2d_42   False\n",
      "147   conv2d_47   False\n",
      "148   batch_normalization_42   False\n",
      "149   batch_normalization_47   False\n",
      "150   activation_42   False\n",
      "151   activation_47   False\n",
      "152   average_pooling2d_4   False\n",
      "153   conv2d_40   False\n",
      "154   conv2d_43   False\n",
      "155   conv2d_48   False\n",
      "156   conv2d_49   False\n",
      "157   batch_normalization_40   False\n",
      "158   batch_normalization_43   False\n",
      "159   batch_normalization_48   False\n",
      "160   batch_normalization_49   False\n",
      "161   activation_40   False\n",
      "162   activation_43   False\n",
      "163   activation_48   False\n",
      "164   activation_49   False\n",
      "165   mixed5   False\n",
      "166   conv2d_54   False\n",
      "167   batch_normalization_54   False\n",
      "168   activation_54   False\n",
      "169   conv2d_55   False\n",
      "170   batch_normalization_55   False\n",
      "171   activation_55   False\n",
      "172   conv2d_51   False\n",
      "173   conv2d_56   False\n",
      "174   batch_normalization_51   False\n",
      "175   batch_normalization_56   False\n",
      "176   activation_51   False\n",
      "177   activation_56   False\n",
      "178   conv2d_52   False\n",
      "179   conv2d_57   False\n",
      "180   batch_normalization_52   False\n",
      "181   batch_normalization_57   False\n",
      "182   activation_52   False\n",
      "183   activation_57   False\n",
      "184   average_pooling2d_5   False\n",
      "185   conv2d_50   False\n",
      "186   conv2d_53   False\n",
      "187   conv2d_58   False\n",
      "188   conv2d_59   False\n",
      "189   batch_normalization_50   False\n",
      "190   batch_normalization_53   False\n",
      "191   batch_normalization_58   False\n",
      "192   batch_normalization_59   False\n",
      "193   activation_50   False\n",
      "194   activation_53   False\n",
      "195   activation_58   False\n",
      "196   activation_59   False\n",
      "197   mixed6   False\n",
      "198   conv2d_64   False\n",
      "199   batch_normalization_64   False\n",
      "200   activation_64   False\n",
      "201   conv2d_65   False\n",
      "202   batch_normalization_65   False\n",
      "203   activation_65   False\n",
      "204   conv2d_61   False\n",
      "205   conv2d_66   False\n",
      "206   batch_normalization_61   False\n",
      "207   batch_normalization_66   False\n",
      "208   activation_61   False\n",
      "209   activation_66   False\n",
      "210   conv2d_62   False\n",
      "211   conv2d_67   False\n",
      "212   batch_normalization_62   False\n",
      "213   batch_normalization_67   False\n",
      "214   activation_62   False\n",
      "215   activation_67   False\n",
      "216   average_pooling2d_6   False\n",
      "217   conv2d_60   False\n",
      "218   conv2d_63   False\n",
      "219   conv2d_68   False\n",
      "220   conv2d_69   False\n",
      "221   batch_normalization_60   False\n",
      "222   batch_normalization_63   False\n",
      "223   batch_normalization_68   False\n",
      "224   batch_normalization_69   False\n",
      "225   activation_60   False\n",
      "226   activation_63   False\n",
      "227   activation_68   False\n",
      "228   activation_69   False\n",
      "229   mixed7   False\n",
      "230   conv2d_72   False\n",
      "231   batch_normalization_72   False\n",
      "232   activation_72   False\n",
      "233   conv2d_73   False\n",
      "234   batch_normalization_73   False\n",
      "235   activation_73   False\n",
      "236   conv2d_70   False\n",
      "237   conv2d_74   False\n",
      "238   batch_normalization_70   False\n",
      "239   batch_normalization_74   False\n",
      "240   activation_70   False\n",
      "241   activation_74   False\n",
      "242   conv2d_71   False\n",
      "243   conv2d_75   False\n",
      "244   batch_normalization_71   False\n",
      "245   batch_normalization_75   False\n",
      "246   activation_71   False\n",
      "247   activation_75   False\n",
      "248   max_pooling2d_3   False\n",
      "249   mixed8   False\n",
      "250   conv2d_80   False\n",
      "251   batch_normalization_80   False\n",
      "252   activation_80   False\n",
      "253   conv2d_77   False\n",
      "254   conv2d_81   False\n",
      "255   batch_normalization_77   False\n",
      "256   batch_normalization_81   False\n",
      "257   activation_77   False\n",
      "258   activation_81   False\n",
      "259   conv2d_78   False\n",
      "260   conv2d_79   False\n",
      "261   conv2d_82   False\n",
      "262   conv2d_83   False\n",
      "263   average_pooling2d_7   False\n",
      "264   conv2d_76   False\n",
      "265   batch_normalization_78   False\n",
      "266   batch_normalization_79   False\n",
      "267   batch_normalization_82   False\n",
      "268   batch_normalization_83   False\n",
      "269   conv2d_84   False\n",
      "270   batch_normalization_76   False\n",
      "271   activation_78   False\n",
      "272   activation_79   False\n",
      "273   activation_82   False\n",
      "274   activation_83   False\n",
      "275   batch_normalization_84   False\n",
      "276   activation_76   False\n",
      "277   mixed9_0   False\n",
      "278   concatenate   False\n",
      "279   activation_84   False\n",
      "280   mixed9   False\n",
      "281   conv2d_89   True\n",
      "282   batch_normalization_89   True\n",
      "283   activation_89   True\n",
      "284   conv2d_86   True\n",
      "285   conv2d_90   True\n",
      "286   batch_normalization_86   True\n",
      "287   batch_normalization_90   True\n",
      "288   activation_86   True\n",
      "289   activation_90   True\n",
      "290   conv2d_87   True\n",
      "291   conv2d_88   True\n",
      "292   conv2d_91   True\n",
      "293   conv2d_92   True\n",
      "294   average_pooling2d_8   True\n",
      "295   conv2d_85   True\n",
      "296   batch_normalization_87   True\n",
      "297   batch_normalization_88   True\n",
      "298   batch_normalization_91   True\n",
      "299   batch_normalization_92   True\n",
      "300   conv2d_93   True\n",
      "301   batch_normalization_85   True\n",
      "302   activation_87   True\n",
      "303   activation_88   True\n",
      "304   activation_91   True\n",
      "305   activation_92   True\n",
      "306   batch_normalization_93   True\n",
      "307   activation_85   True\n",
      "308   mixed9_1   True\n",
      "309   concatenate_1   True\n",
      "310   activation_93   True\n",
      "311   mixed10   True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model retrieved\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Functional)    (None, 5, 5, 2048)        21802784  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 51200)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              52429824  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 76,342,058\n",
      "Trainable params: 60,612,810\n",
      "Non-trainable params: 15,729,248\n",
      "_________________________________________________________________\n",
      "model building done\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 361s 4s/step - loss: 7.6131 - accuracy: 0.2959 - val_loss: 0.9985 - val_accuracy: 0.7000\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 341s 4s/step - loss: 0.8550 - accuracy: 0.7229 - val_loss: 1.0006 - val_accuracy: 0.6922\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 330s 4s/step - loss: 0.7114 - accuracy: 0.7624 - val_loss: 0.8370 - val_accuracy: 0.7219\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 332s 4s/step - loss: 0.6160 - accuracy: 0.8052 - val_loss: 1.0196 - val_accuracy: 0.7203\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 329s 4s/step - loss: 0.5555 - accuracy: 0.8176 - val_loss: 1.0300 - val_accuracy: 0.7203\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 329s 4s/step - loss: 0.4668 - accuracy: 0.8513 - val_loss: 1.0366 - val_accuracy: 0.7250\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 332s 4s/step - loss: 0.4610 - accuracy: 0.8487 - val_loss: 0.9233 - val_accuracy: 0.7563\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 328s 4s/step - loss: 0.3801 - accuracy: 0.8717 - val_loss: 0.9387 - val_accuracy: 0.7563\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 329s 4s/step - loss: 0.3514 - accuracy: 0.8909 - val_loss: 1.0662 - val_accuracy: 0.7250\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 330s 4s/step - loss: 0.3105 - accuracy: 0.8964 - val_loss: 0.9489 - val_accuracy: 0.7547\n",
      "32/32 [==============================] - 97s 3s/step - loss: 0.9380 - accuracy: 0.7430\n",
      "model training done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 28519<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210413_031104-3bwr0vwf/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210413_031104-3bwr0vwf/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.32332</td></tr><tr><td>accuracy</td><td>0.89174</td></tr><tr><td>val_loss</td><td>0.94892</td></tr><tr><td>val_accuracy</td><td>0.75469</td></tr><tr><td>_runtime</td><td>3453</td></tr><tr><td>_timestamp</td><td>1618267117</td></tr><tr><td>_step</td><td>10</td></tr><tr><td>best_val_accuracy</td><td>0.75625</td></tr><tr><td>best_epoch</td><td>6</td></tr><tr><td>test_acc </td><td>0.743</td></tr><tr><td>test_loss </td><td>0.93804</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▂▂▂▂▁▁▁▁▁</td></tr><tr><td>accuracy</td><td>▁▅▆▆▇▇▇███</td></tr><tr><td>val_loss</td><td>▆▆▁▇▇▇▄▄█▄</td></tr><tr><td>val_accuracy</td><td>▂▁▄▄▄▅██▅█</td></tr><tr><td>_runtime</td><td>▁▂▃▃▄▅▆▆▇██</td></tr><tr><td>_timestamp</td><td>▁▂▃▃▄▅▆▆▇██</td></tr><tr><td>_step</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>test_acc </td><td>▁</td></tr><tr><td>test_loss </td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">cs6910-a2-partB</strong>: <a href=\"https://wandb.ai/notarchana/cs6910-a2/runs/3bwr0vwf\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/runs/3bwr0vwf</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fc0kocu6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: ['InceptionV3', 249, 2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.25<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">cs6910-a2-partB</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/notarchana/cs6910-a2\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/notarchana/cs6910-a2/sweeps/wyy702q1\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/sweeps/wyy702q1</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/notarchana/cs6910-a2/runs/fc0kocu6\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/runs/fc0kocu6</a><br/>\n",
       "                Run data is saved locally in <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210413_041024-fc0kocu6</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249\n",
      "1   input_1   False\n",
      "2   conv2d   False\n",
      "3   batch_normalization   False\n",
      "4   activation   False\n",
      "5   conv2d_1   False\n",
      "6   batch_normalization_1   False\n",
      "7   activation_1   False\n",
      "8   conv2d_2   False\n",
      "9   batch_normalization_2   False\n",
      "10   activation_2   False\n",
      "11   max_pooling2d   False\n",
      "12   conv2d_3   False\n",
      "13   batch_normalization_3   False\n",
      "14   activation_3   False\n",
      "15   conv2d_4   False\n",
      "16   batch_normalization_4   False\n",
      "17   activation_4   False\n",
      "18   max_pooling2d_1   False\n",
      "19   conv2d_8   False\n",
      "20   batch_normalization_8   False\n",
      "21   activation_8   False\n",
      "22   conv2d_6   False\n",
      "23   conv2d_9   False\n",
      "24   batch_normalization_6   False\n",
      "25   batch_normalization_9   False\n",
      "26   activation_6   False\n",
      "27   activation_9   False\n",
      "28   average_pooling2d   False\n",
      "29   conv2d_5   False\n",
      "30   conv2d_7   False\n",
      "31   conv2d_10   False\n",
      "32   conv2d_11   False\n",
      "33   batch_normalization_5   False\n",
      "34   batch_normalization_7   False\n",
      "35   batch_normalization_10   False\n",
      "36   batch_normalization_11   False\n",
      "37   activation_5   False\n",
      "38   activation_7   False\n",
      "39   activation_10   False\n",
      "40   activation_11   False\n",
      "41   mixed0   False\n",
      "42   conv2d_15   False\n",
      "43   batch_normalization_15   False\n",
      "44   activation_15   False\n",
      "45   conv2d_13   False\n",
      "46   conv2d_16   False\n",
      "47   batch_normalization_13   False\n",
      "48   batch_normalization_16   False\n",
      "49   activation_13   False\n",
      "50   activation_16   False\n",
      "51   average_pooling2d_1   False\n",
      "52   conv2d_12   False\n",
      "53   conv2d_14   False\n",
      "54   conv2d_17   False\n",
      "55   conv2d_18   False\n",
      "56   batch_normalization_12   False\n",
      "57   batch_normalization_14   False\n",
      "58   batch_normalization_17   False\n",
      "59   batch_normalization_18   False\n",
      "60   activation_12   False\n",
      "61   activation_14   False\n",
      "62   activation_17   False\n",
      "63   activation_18   False\n",
      "64   mixed1   False\n",
      "65   conv2d_22   False\n",
      "66   batch_normalization_22   False\n",
      "67   activation_22   False\n",
      "68   conv2d_20   False\n",
      "69   conv2d_23   False\n",
      "70   batch_normalization_20   False\n",
      "71   batch_normalization_23   False\n",
      "72   activation_20   False\n",
      "73   activation_23   False\n",
      "74   average_pooling2d_2   False\n",
      "75   conv2d_19   False\n",
      "76   conv2d_21   False\n",
      "77   conv2d_24   False\n",
      "78   conv2d_25   False\n",
      "79   batch_normalization_19   False\n",
      "80   batch_normalization_21   False\n",
      "81   batch_normalization_24   False\n",
      "82   batch_normalization_25   False\n",
      "83   activation_19   False\n",
      "84   activation_21   False\n",
      "85   activation_24   False\n",
      "86   activation_25   False\n",
      "87   mixed2   False\n",
      "88   conv2d_27   False\n",
      "89   batch_normalization_27   False\n",
      "90   activation_27   False\n",
      "91   conv2d_28   False\n",
      "92   batch_normalization_28   False\n",
      "93   activation_28   False\n",
      "94   conv2d_26   False\n",
      "95   conv2d_29   False\n",
      "96   batch_normalization_26   False\n",
      "97   batch_normalization_29   False\n",
      "98   activation_26   False\n",
      "99   activation_29   False\n",
      "100   max_pooling2d_2   False\n",
      "101   mixed3   False\n",
      "102   conv2d_34   False\n",
      "103   batch_normalization_34   False\n",
      "104   activation_34   False\n",
      "105   conv2d_35   False\n",
      "106   batch_normalization_35   False\n",
      "107   activation_35   False\n",
      "108   conv2d_31   False\n",
      "109   conv2d_36   False\n",
      "110   batch_normalization_31   False\n",
      "111   batch_normalization_36   False\n",
      "112   activation_31   False\n",
      "113   activation_36   False\n",
      "114   conv2d_32   False\n",
      "115   conv2d_37   False\n",
      "116   batch_normalization_32   False\n",
      "117   batch_normalization_37   False\n",
      "118   activation_32   False\n",
      "119   activation_37   False\n",
      "120   average_pooling2d_3   False\n",
      "121   conv2d_30   False\n",
      "122   conv2d_33   False\n",
      "123   conv2d_38   False\n",
      "124   conv2d_39   False\n",
      "125   batch_normalization_30   False\n",
      "126   batch_normalization_33   False\n",
      "127   batch_normalization_38   False\n",
      "128   batch_normalization_39   False\n",
      "129   activation_30   False\n",
      "130   activation_33   False\n",
      "131   activation_38   False\n",
      "132   activation_39   False\n",
      "133   mixed4   False\n",
      "134   conv2d_44   False\n",
      "135   batch_normalization_44   False\n",
      "136   activation_44   False\n",
      "137   conv2d_45   False\n",
      "138   batch_normalization_45   False\n",
      "139   activation_45   False\n",
      "140   conv2d_41   False\n",
      "141   conv2d_46   False\n",
      "142   batch_normalization_41   False\n",
      "143   batch_normalization_46   False\n",
      "144   activation_41   False\n",
      "145   activation_46   False\n",
      "146   conv2d_42   False\n",
      "147   conv2d_47   False\n",
      "148   batch_normalization_42   False\n",
      "149   batch_normalization_47   False\n",
      "150   activation_42   False\n",
      "151   activation_47   False\n",
      "152   average_pooling2d_4   False\n",
      "153   conv2d_40   False\n",
      "154   conv2d_43   False\n",
      "155   conv2d_48   False\n",
      "156   conv2d_49   False\n",
      "157   batch_normalization_40   False\n",
      "158   batch_normalization_43   False\n",
      "159   batch_normalization_48   False\n",
      "160   batch_normalization_49   False\n",
      "161   activation_40   False\n",
      "162   activation_43   False\n",
      "163   activation_48   False\n",
      "164   activation_49   False\n",
      "165   mixed5   False\n",
      "166   conv2d_54   False\n",
      "167   batch_normalization_54   False\n",
      "168   activation_54   False\n",
      "169   conv2d_55   False\n",
      "170   batch_normalization_55   False\n",
      "171   activation_55   False\n",
      "172   conv2d_51   False\n",
      "173   conv2d_56   False\n",
      "174   batch_normalization_51   False\n",
      "175   batch_normalization_56   False\n",
      "176   activation_51   False\n",
      "177   activation_56   False\n",
      "178   conv2d_52   False\n",
      "179   conv2d_57   False\n",
      "180   batch_normalization_52   False\n",
      "181   batch_normalization_57   False\n",
      "182   activation_52   False\n",
      "183   activation_57   False\n",
      "184   average_pooling2d_5   False\n",
      "185   conv2d_50   False\n",
      "186   conv2d_53   False\n",
      "187   conv2d_58   False\n",
      "188   conv2d_59   False\n",
      "189   batch_normalization_50   False\n",
      "190   batch_normalization_53   False\n",
      "191   batch_normalization_58   False\n",
      "192   batch_normalization_59   False\n",
      "193   activation_50   False\n",
      "194   activation_53   False\n",
      "195   activation_58   False\n",
      "196   activation_59   False\n",
      "197   mixed6   False\n",
      "198   conv2d_64   False\n",
      "199   batch_normalization_64   False\n",
      "200   activation_64   False\n",
      "201   conv2d_65   False\n",
      "202   batch_normalization_65   False\n",
      "203   activation_65   False\n",
      "204   conv2d_61   False\n",
      "205   conv2d_66   False\n",
      "206   batch_normalization_61   False\n",
      "207   batch_normalization_66   False\n",
      "208   activation_61   False\n",
      "209   activation_66   False\n",
      "210   conv2d_62   False\n",
      "211   conv2d_67   False\n",
      "212   batch_normalization_62   False\n",
      "213   batch_normalization_67   False\n",
      "214   activation_62   False\n",
      "215   activation_67   False\n",
      "216   average_pooling2d_6   False\n",
      "217   conv2d_60   False\n",
      "218   conv2d_63   False\n",
      "219   conv2d_68   False\n",
      "220   conv2d_69   False\n",
      "221   batch_normalization_60   False\n",
      "222   batch_normalization_63   False\n",
      "223   batch_normalization_68   False\n",
      "224   batch_normalization_69   False\n",
      "225   activation_60   False\n",
      "226   activation_63   False\n",
      "227   activation_68   False\n",
      "228   activation_69   False\n",
      "229   mixed7   False\n",
      "230   conv2d_72   False\n",
      "231   batch_normalization_72   False\n",
      "232   activation_72   False\n",
      "233   conv2d_73   False\n",
      "234   batch_normalization_73   False\n",
      "235   activation_73   False\n",
      "236   conv2d_70   False\n",
      "237   conv2d_74   False\n",
      "238   batch_normalization_70   False\n",
      "239   batch_normalization_74   False\n",
      "240   activation_70   False\n",
      "241   activation_74   False\n",
      "242   conv2d_71   False\n",
      "243   conv2d_75   False\n",
      "244   batch_normalization_71   False\n",
      "245   batch_normalization_75   False\n",
      "246   activation_71   False\n",
      "247   activation_75   False\n",
      "248   max_pooling2d_3   False\n",
      "249   mixed8   False\n",
      "250   conv2d_80   True\n",
      "251   batch_normalization_80   True\n",
      "252   activation_80   True\n",
      "253   conv2d_77   True\n",
      "254   conv2d_81   True\n",
      "255   batch_normalization_77   True\n",
      "256   batch_normalization_81   True\n",
      "257   activation_77   True\n",
      "258   activation_81   True\n",
      "259   conv2d_78   True\n",
      "260   conv2d_79   True\n",
      "261   conv2d_82   True\n",
      "262   conv2d_83   True\n",
      "263   average_pooling2d_7   True\n",
      "264   conv2d_76   True\n",
      "265   batch_normalization_78   True\n",
      "266   batch_normalization_79   True\n",
      "267   batch_normalization_82   True\n",
      "268   batch_normalization_83   True\n",
      "269   conv2d_84   True\n",
      "270   batch_normalization_76   True\n",
      "271   activation_78   True\n",
      "272   activation_79   True\n",
      "273   activation_82   True\n",
      "274   activation_83   True\n",
      "275   batch_normalization_84   True\n",
      "276   activation_76   True\n",
      "277   mixed9_0   True\n",
      "278   concatenate   True\n",
      "279   activation_84   True\n",
      "280   mixed9   True\n",
      "281   conv2d_89   True\n",
      "282   batch_normalization_89   True\n",
      "283   activation_89   True\n",
      "284   conv2d_86   True\n",
      "285   conv2d_90   True\n",
      "286   batch_normalization_86   True\n",
      "287   batch_normalization_90   True\n",
      "288   activation_86   True\n",
      "289   activation_90   True\n",
      "290   conv2d_87   True\n",
      "291   conv2d_88   True\n",
      "292   conv2d_91   True\n",
      "293   conv2d_92   True\n",
      "294   average_pooling2d_8   True\n",
      "295   conv2d_85   True\n",
      "296   batch_normalization_87   True\n",
      "297   batch_normalization_88   True\n",
      "298   batch_normalization_91   True\n",
      "299   batch_normalization_92   True\n",
      "300   conv2d_93   True\n",
      "301   batch_normalization_85   True\n",
      "302   activation_87   True\n",
      "303   activation_88   True\n",
      "304   activation_91   True\n",
      "305   activation_92   True\n",
      "306   batch_normalization_93   True\n",
      "307   activation_85   True\n",
      "308   mixed9_1   True\n",
      "309   concatenate_1   True\n",
      "310   activation_93   True\n",
      "311   mixed10   True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model retrieved\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Functional)    (None, 5, 5, 2048)        21802784  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 51200)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              52429824  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 76,342,058\n",
      "Trainable params: 65,654,154\n",
      "Non-trainable params: 10,687,904\n",
      "_________________________________________________________________\n",
      "model building done\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 354s 4s/step - loss: 4.6956 - accuracy: 0.2596 - val_loss: 1.7053 - val_accuracy: 0.5266\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 353s 4s/step - loss: 1.0403 - accuracy: 0.6430 - val_loss: 1.1952 - val_accuracy: 0.6797\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 351s 4s/step - loss: 0.8463 - accuracy: 0.7285 - val_loss: 0.9605 - val_accuracy: 0.6828\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 356s 4s/step - loss: 0.6870 - accuracy: 0.7802 - val_loss: 0.9887 - val_accuracy: 0.6875\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 353s 4s/step - loss: 0.6109 - accuracy: 0.8104 - val_loss: 0.9360 - val_accuracy: 0.7234\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 353s 4s/step - loss: 0.5336 - accuracy: 0.8297 - val_loss: 0.9871 - val_accuracy: 0.7078\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 353s 4s/step - loss: 0.4910 - accuracy: 0.8405 - val_loss: 0.9536 - val_accuracy: 0.7328\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 353s 4s/step - loss: 0.4772 - accuracy: 0.8504 - val_loss: 0.9675 - val_accuracy: 0.7312\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 352s 4s/step - loss: 0.4073 - accuracy: 0.8700 - val_loss: 1.0814 - val_accuracy: 0.7297\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 352s 4s/step - loss: 0.3923 - accuracy: 0.8772 - val_loss: 1.0921 - val_accuracy: 0.7219\n",
      "32/32 [==============================] - 93s 3s/step - loss: 0.9596 - accuracy: 0.7420\n",
      "model training done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 29130<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210413_041024-fc0kocu6/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210413_041024-fc0kocu6/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.40677</td></tr><tr><td>accuracy</td><td>0.87227</td></tr><tr><td>val_loss</td><td>1.09213</td></tr><tr><td>val_accuracy</td><td>0.72188</td></tr><tr><td>_runtime</td><td>3640</td></tr><tr><td>_timestamp</td><td>1618270864</td></tr><tr><td>_step</td><td>10</td></tr><tr><td>best_val_accuracy</td><td>0.73281</td></tr><tr><td>best_epoch</td><td>6</td></tr><tr><td>test_acc </td><td>0.742</td></tr><tr><td>test_loss </td><td>0.95957</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▃▂▂▂▂▁▁▁▁</td></tr><tr><td>accuracy</td><td>▁▅▆▆▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▁▁▁▁▁▁▂▂</td></tr><tr><td>val_accuracy</td><td>▁▆▆▆█▇████</td></tr><tr><td>_runtime</td><td>▁▂▃▃▄▅▆▆▇██</td></tr><tr><td>_timestamp</td><td>▁▂▃▃▄▅▆▆▇██</td></tr><tr><td>_step</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>test_acc </td><td>▁</td></tr><tr><td>test_loss </td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">cs6910-a2-partB</strong>: <a href=\"https://wandb.ai/notarchana/cs6910-a2/runs/fc0kocu6\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/runs/fc0kocu6</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yik4f94z with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: ['InceptionResNetV2', 779, 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.25<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">cs6910-a2-partB</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/notarchana/cs6910-a2\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/notarchana/cs6910-a2/sweeps/wyy702q1\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/sweeps/wyy702q1</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/notarchana/cs6910-a2/runs/yik4f94z\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/runs/yik4f94z</a><br/>\n",
       "                Run data is saved locally in <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210413_051242-yik4f94z</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779\n",
      "1   input_1   False\n",
      "2   conv2d   False\n",
      "3   batch_normalization   False\n",
      "4   activation   False\n",
      "5   conv2d_1   False\n",
      "6   batch_normalization_1   False\n",
      "7   activation_1   False\n",
      "8   conv2d_2   False\n",
      "9   batch_normalization_2   False\n",
      "10   activation_2   False\n",
      "11   max_pooling2d   False\n",
      "12   conv2d_3   False\n",
      "13   batch_normalization_3   False\n",
      "14   activation_3   False\n",
      "15   conv2d_4   False\n",
      "16   batch_normalization_4   False\n",
      "17   activation_4   False\n",
      "18   max_pooling2d_1   False\n",
      "19   conv2d_8   False\n",
      "20   batch_normalization_8   False\n",
      "21   activation_8   False\n",
      "22   conv2d_6   False\n",
      "23   conv2d_9   False\n",
      "24   batch_normalization_6   False\n",
      "25   batch_normalization_9   False\n",
      "26   activation_6   False\n",
      "27   activation_9   False\n",
      "28   average_pooling2d   False\n",
      "29   conv2d_5   False\n",
      "30   conv2d_7   False\n",
      "31   conv2d_10   False\n",
      "32   conv2d_11   False\n",
      "33   batch_normalization_5   False\n",
      "34   batch_normalization_7   False\n",
      "35   batch_normalization_10   False\n",
      "36   batch_normalization_11   False\n",
      "37   activation_5   False\n",
      "38   activation_7   False\n",
      "39   activation_10   False\n",
      "40   activation_11   False\n",
      "41   mixed_5b   False\n",
      "42   conv2d_15   False\n",
      "43   batch_normalization_15   False\n",
      "44   activation_15   False\n",
      "45   conv2d_13   False\n",
      "46   conv2d_16   False\n",
      "47   batch_normalization_13   False\n",
      "48   batch_normalization_16   False\n",
      "49   activation_13   False\n",
      "50   activation_16   False\n",
      "51   conv2d_12   False\n",
      "52   conv2d_14   False\n",
      "53   conv2d_17   False\n",
      "54   batch_normalization_12   False\n",
      "55   batch_normalization_14   False\n",
      "56   batch_normalization_17   False\n",
      "57   activation_12   False\n",
      "58   activation_14   False\n",
      "59   activation_17   False\n",
      "60   block35_1_mixed   False\n",
      "61   block35_1_conv   False\n",
      "62   block35_1   False\n",
      "63   block35_1_ac   False\n",
      "64   conv2d_21   False\n",
      "65   batch_normalization_21   False\n",
      "66   activation_21   False\n",
      "67   conv2d_19   False\n",
      "68   conv2d_22   False\n",
      "69   batch_normalization_19   False\n",
      "70   batch_normalization_22   False\n",
      "71   activation_19   False\n",
      "72   activation_22   False\n",
      "73   conv2d_18   False\n",
      "74   conv2d_20   False\n",
      "75   conv2d_23   False\n",
      "76   batch_normalization_18   False\n",
      "77   batch_normalization_20   False\n",
      "78   batch_normalization_23   False\n",
      "79   activation_18   False\n",
      "80   activation_20   False\n",
      "81   activation_23   False\n",
      "82   block35_2_mixed   False\n",
      "83   block35_2_conv   False\n",
      "84   block35_2   False\n",
      "85   block35_2_ac   False\n",
      "86   conv2d_27   False\n",
      "87   batch_normalization_27   False\n",
      "88   activation_27   False\n",
      "89   conv2d_25   False\n",
      "90   conv2d_28   False\n",
      "91   batch_normalization_25   False\n",
      "92   batch_normalization_28   False\n",
      "93   activation_25   False\n",
      "94   activation_28   False\n",
      "95   conv2d_24   False\n",
      "96   conv2d_26   False\n",
      "97   conv2d_29   False\n",
      "98   batch_normalization_24   False\n",
      "99   batch_normalization_26   False\n",
      "100   batch_normalization_29   False\n",
      "101   activation_24   False\n",
      "102   activation_26   False\n",
      "103   activation_29   False\n",
      "104   block35_3_mixed   False\n",
      "105   block35_3_conv   False\n",
      "106   block35_3   False\n",
      "107   block35_3_ac   False\n",
      "108   conv2d_33   False\n",
      "109   batch_normalization_33   False\n",
      "110   activation_33   False\n",
      "111   conv2d_31   False\n",
      "112   conv2d_34   False\n",
      "113   batch_normalization_31   False\n",
      "114   batch_normalization_34   False\n",
      "115   activation_31   False\n",
      "116   activation_34   False\n",
      "117   conv2d_30   False\n",
      "118   conv2d_32   False\n",
      "119   conv2d_35   False\n",
      "120   batch_normalization_30   False\n",
      "121   batch_normalization_32   False\n",
      "122   batch_normalization_35   False\n",
      "123   activation_30   False\n",
      "124   activation_32   False\n",
      "125   activation_35   False\n",
      "126   block35_4_mixed   False\n",
      "127   block35_4_conv   False\n",
      "128   block35_4   False\n",
      "129   block35_4_ac   False\n",
      "130   conv2d_39   False\n",
      "131   batch_normalization_39   False\n",
      "132   activation_39   False\n",
      "133   conv2d_37   False\n",
      "134   conv2d_40   False\n",
      "135   batch_normalization_37   False\n",
      "136   batch_normalization_40   False\n",
      "137   activation_37   False\n",
      "138   activation_40   False\n",
      "139   conv2d_36   False\n",
      "140   conv2d_38   False\n",
      "141   conv2d_41   False\n",
      "142   batch_normalization_36   False\n",
      "143   batch_normalization_38   False\n",
      "144   batch_normalization_41   False\n",
      "145   activation_36   False\n",
      "146   activation_38   False\n",
      "147   activation_41   False\n",
      "148   block35_5_mixed   False\n",
      "149   block35_5_conv   False\n",
      "150   block35_5   False\n",
      "151   block35_5_ac   False\n",
      "152   conv2d_45   False\n",
      "153   batch_normalization_45   False\n",
      "154   activation_45   False\n",
      "155   conv2d_43   False\n",
      "156   conv2d_46   False\n",
      "157   batch_normalization_43   False\n",
      "158   batch_normalization_46   False\n",
      "159   activation_43   False\n",
      "160   activation_46   False\n",
      "161   conv2d_42   False\n",
      "162   conv2d_44   False\n",
      "163   conv2d_47   False\n",
      "164   batch_normalization_42   False\n",
      "165   batch_normalization_44   False\n",
      "166   batch_normalization_47   False\n",
      "167   activation_42   False\n",
      "168   activation_44   False\n",
      "169   activation_47   False\n",
      "170   block35_6_mixed   False\n",
      "171   block35_6_conv   False\n",
      "172   block35_6   False\n",
      "173   block35_6_ac   False\n",
      "174   conv2d_51   False\n",
      "175   batch_normalization_51   False\n",
      "176   activation_51   False\n",
      "177   conv2d_49   False\n",
      "178   conv2d_52   False\n",
      "179   batch_normalization_49   False\n",
      "180   batch_normalization_52   False\n",
      "181   activation_49   False\n",
      "182   activation_52   False\n",
      "183   conv2d_48   False\n",
      "184   conv2d_50   False\n",
      "185   conv2d_53   False\n",
      "186   batch_normalization_48   False\n",
      "187   batch_normalization_50   False\n",
      "188   batch_normalization_53   False\n",
      "189   activation_48   False\n",
      "190   activation_50   False\n",
      "191   activation_53   False\n",
      "192   block35_7_mixed   False\n",
      "193   block35_7_conv   False\n",
      "194   block35_7   False\n",
      "195   block35_7_ac   False\n",
      "196   conv2d_57   False\n",
      "197   batch_normalization_57   False\n",
      "198   activation_57   False\n",
      "199   conv2d_55   False\n",
      "200   conv2d_58   False\n",
      "201   batch_normalization_55   False\n",
      "202   batch_normalization_58   False\n",
      "203   activation_55   False\n",
      "204   activation_58   False\n",
      "205   conv2d_54   False\n",
      "206   conv2d_56   False\n",
      "207   conv2d_59   False\n",
      "208   batch_normalization_54   False\n",
      "209   batch_normalization_56   False\n",
      "210   batch_normalization_59   False\n",
      "211   activation_54   False\n",
      "212   activation_56   False\n",
      "213   activation_59   False\n",
      "214   block35_8_mixed   False\n",
      "215   block35_8_conv   False\n",
      "216   block35_8   False\n",
      "217   block35_8_ac   False\n",
      "218   conv2d_63   False\n",
      "219   batch_normalization_63   False\n",
      "220   activation_63   False\n",
      "221   conv2d_61   False\n",
      "222   conv2d_64   False\n",
      "223   batch_normalization_61   False\n",
      "224   batch_normalization_64   False\n",
      "225   activation_61   False\n",
      "226   activation_64   False\n",
      "227   conv2d_60   False\n",
      "228   conv2d_62   False\n",
      "229   conv2d_65   False\n",
      "230   batch_normalization_60   False\n",
      "231   batch_normalization_62   False\n",
      "232   batch_normalization_65   False\n",
      "233   activation_60   False\n",
      "234   activation_62   False\n",
      "235   activation_65   False\n",
      "236   block35_9_mixed   False\n",
      "237   block35_9_conv   False\n",
      "238   block35_9   False\n",
      "239   block35_9_ac   False\n",
      "240   conv2d_69   False\n",
      "241   batch_normalization_69   False\n",
      "242   activation_69   False\n",
      "243   conv2d_67   False\n",
      "244   conv2d_70   False\n",
      "245   batch_normalization_67   False\n",
      "246   batch_normalization_70   False\n",
      "247   activation_67   False\n",
      "248   activation_70   False\n",
      "249   conv2d_66   False\n",
      "250   conv2d_68   False\n",
      "251   conv2d_71   False\n",
      "252   batch_normalization_66   False\n",
      "253   batch_normalization_68   False\n",
      "254   batch_normalization_71   False\n",
      "255   activation_66   False\n",
      "256   activation_68   False\n",
      "257   activation_71   False\n",
      "258   block35_10_mixed   False\n",
      "259   block35_10_conv   False\n",
      "260   block35_10   False\n",
      "261   block35_10_ac   False\n",
      "262   conv2d_73   False\n",
      "263   batch_normalization_73   False\n",
      "264   activation_73   False\n",
      "265   conv2d_74   False\n",
      "266   batch_normalization_74   False\n",
      "267   activation_74   False\n",
      "268   conv2d_72   False\n",
      "269   conv2d_75   False\n",
      "270   batch_normalization_72   False\n",
      "271   batch_normalization_75   False\n",
      "272   activation_72   False\n",
      "273   activation_75   False\n",
      "274   max_pooling2d_2   False\n",
      "275   mixed_6a   False\n",
      "276   conv2d_77   False\n",
      "277   batch_normalization_77   False\n",
      "278   activation_77   False\n",
      "279   conv2d_78   False\n",
      "280   batch_normalization_78   False\n",
      "281   activation_78   False\n",
      "282   conv2d_76   False\n",
      "283   conv2d_79   False\n",
      "284   batch_normalization_76   False\n",
      "285   batch_normalization_79   False\n",
      "286   activation_76   False\n",
      "287   activation_79   False\n",
      "288   block17_1_mixed   False\n",
      "289   block17_1_conv   False\n",
      "290   block17_1   False\n",
      "291   block17_1_ac   False\n",
      "292   conv2d_81   False\n",
      "293   batch_normalization_81   False\n",
      "294   activation_81   False\n",
      "295   conv2d_82   False\n",
      "296   batch_normalization_82   False\n",
      "297   activation_82   False\n",
      "298   conv2d_80   False\n",
      "299   conv2d_83   False\n",
      "300   batch_normalization_80   False\n",
      "301   batch_normalization_83   False\n",
      "302   activation_80   False\n",
      "303   activation_83   False\n",
      "304   block17_2_mixed   False\n",
      "305   block17_2_conv   False\n",
      "306   block17_2   False\n",
      "307   block17_2_ac   False\n",
      "308   conv2d_85   False\n",
      "309   batch_normalization_85   False\n",
      "310   activation_85   False\n",
      "311   conv2d_86   False\n",
      "312   batch_normalization_86   False\n",
      "313   activation_86   False\n",
      "314   conv2d_84   False\n",
      "315   conv2d_87   False\n",
      "316   batch_normalization_84   False\n",
      "317   batch_normalization_87   False\n",
      "318   activation_84   False\n",
      "319   activation_87   False\n",
      "320   block17_3_mixed   False\n",
      "321   block17_3_conv   False\n",
      "322   block17_3   False\n",
      "323   block17_3_ac   False\n",
      "324   conv2d_89   False\n",
      "325   batch_normalization_89   False\n",
      "326   activation_89   False\n",
      "327   conv2d_90   False\n",
      "328   batch_normalization_90   False\n",
      "329   activation_90   False\n",
      "330   conv2d_88   False\n",
      "331   conv2d_91   False\n",
      "332   batch_normalization_88   False\n",
      "333   batch_normalization_91   False\n",
      "334   activation_88   False\n",
      "335   activation_91   False\n",
      "336   block17_4_mixed   False\n",
      "337   block17_4_conv   False\n",
      "338   block17_4   False\n",
      "339   block17_4_ac   False\n",
      "340   conv2d_93   False\n",
      "341   batch_normalization_93   False\n",
      "342   activation_93   False\n",
      "343   conv2d_94   False\n",
      "344   batch_normalization_94   False\n",
      "345   activation_94   False\n",
      "346   conv2d_92   False\n",
      "347   conv2d_95   False\n",
      "348   batch_normalization_92   False\n",
      "349   batch_normalization_95   False\n",
      "350   activation_92   False\n",
      "351   activation_95   False\n",
      "352   block17_5_mixed   False\n",
      "353   block17_5_conv   False\n",
      "354   block17_5   False\n",
      "355   block17_5_ac   False\n",
      "356   conv2d_97   False\n",
      "357   batch_normalization_97   False\n",
      "358   activation_97   False\n",
      "359   conv2d_98   False\n",
      "360   batch_normalization_98   False\n",
      "361   activation_98   False\n",
      "362   conv2d_96   False\n",
      "363   conv2d_99   False\n",
      "364   batch_normalization_96   False\n",
      "365   batch_normalization_99   False\n",
      "366   activation_96   False\n",
      "367   activation_99   False\n",
      "368   block17_6_mixed   False\n",
      "369   block17_6_conv   False\n",
      "370   block17_6   False\n",
      "371   block17_6_ac   False\n",
      "372   conv2d_101   False\n",
      "373   batch_normalization_101   False\n",
      "374   activation_101   False\n",
      "375   conv2d_102   False\n",
      "376   batch_normalization_102   False\n",
      "377   activation_102   False\n",
      "378   conv2d_100   False\n",
      "379   conv2d_103   False\n",
      "380   batch_normalization_100   False\n",
      "381   batch_normalization_103   False\n",
      "382   activation_100   False\n",
      "383   activation_103   False\n",
      "384   block17_7_mixed   False\n",
      "385   block17_7_conv   False\n",
      "386   block17_7   False\n",
      "387   block17_7_ac   False\n",
      "388   conv2d_105   False\n",
      "389   batch_normalization_105   False\n",
      "390   activation_105   False\n",
      "391   conv2d_106   False\n",
      "392   batch_normalization_106   False\n",
      "393   activation_106   False\n",
      "394   conv2d_104   False\n",
      "395   conv2d_107   False\n",
      "396   batch_normalization_104   False\n",
      "397   batch_normalization_107   False\n",
      "398   activation_104   False\n",
      "399   activation_107   False\n",
      "400   block17_8_mixed   False\n",
      "401   block17_8_conv   False\n",
      "402   block17_8   False\n",
      "403   block17_8_ac   False\n",
      "404   conv2d_109   False\n",
      "405   batch_normalization_109   False\n",
      "406   activation_109   False\n",
      "407   conv2d_110   False\n",
      "408   batch_normalization_110   False\n",
      "409   activation_110   False\n",
      "410   conv2d_108   False\n",
      "411   conv2d_111   False\n",
      "412   batch_normalization_108   False\n",
      "413   batch_normalization_111   False\n",
      "414   activation_108   False\n",
      "415   activation_111   False\n",
      "416   block17_9_mixed   False\n",
      "417   block17_9_conv   False\n",
      "418   block17_9   False\n",
      "419   block17_9_ac   False\n",
      "420   conv2d_113   False\n",
      "421   batch_normalization_113   False\n",
      "422   activation_113   False\n",
      "423   conv2d_114   False\n",
      "424   batch_normalization_114   False\n",
      "425   activation_114   False\n",
      "426   conv2d_112   False\n",
      "427   conv2d_115   False\n",
      "428   batch_normalization_112   False\n",
      "429   batch_normalization_115   False\n",
      "430   activation_112   False\n",
      "431   activation_115   False\n",
      "432   block17_10_mixed   False\n",
      "433   block17_10_conv   False\n",
      "434   block17_10   False\n",
      "435   block17_10_ac   False\n",
      "436   conv2d_117   False\n",
      "437   batch_normalization_117   False\n",
      "438   activation_117   False\n",
      "439   conv2d_118   False\n",
      "440   batch_normalization_118   False\n",
      "441   activation_118   False\n",
      "442   conv2d_116   False\n",
      "443   conv2d_119   False\n",
      "444   batch_normalization_116   False\n",
      "445   batch_normalization_119   False\n",
      "446   activation_116   False\n",
      "447   activation_119   False\n",
      "448   block17_11_mixed   False\n",
      "449   block17_11_conv   False\n",
      "450   block17_11   False\n",
      "451   block17_11_ac   False\n",
      "452   conv2d_121   False\n",
      "453   batch_normalization_121   False\n",
      "454   activation_121   False\n",
      "455   conv2d_122   False\n",
      "456   batch_normalization_122   False\n",
      "457   activation_122   False\n",
      "458   conv2d_120   False\n",
      "459   conv2d_123   False\n",
      "460   batch_normalization_120   False\n",
      "461   batch_normalization_123   False\n",
      "462   activation_120   False\n",
      "463   activation_123   False\n",
      "464   block17_12_mixed   False\n",
      "465   block17_12_conv   False\n",
      "466   block17_12   False\n",
      "467   block17_12_ac   False\n",
      "468   conv2d_125   False\n",
      "469   batch_normalization_125   False\n",
      "470   activation_125   False\n",
      "471   conv2d_126   False\n",
      "472   batch_normalization_126   False\n",
      "473   activation_126   False\n",
      "474   conv2d_124   False\n",
      "475   conv2d_127   False\n",
      "476   batch_normalization_124   False\n",
      "477   batch_normalization_127   False\n",
      "478   activation_124   False\n",
      "479   activation_127   False\n",
      "480   block17_13_mixed   False\n",
      "481   block17_13_conv   False\n",
      "482   block17_13   False\n",
      "483   block17_13_ac   False\n",
      "484   conv2d_129   False\n",
      "485   batch_normalization_129   False\n",
      "486   activation_129   False\n",
      "487   conv2d_130   False\n",
      "488   batch_normalization_130   False\n",
      "489   activation_130   False\n",
      "490   conv2d_128   False\n",
      "491   conv2d_131   False\n",
      "492   batch_normalization_128   False\n",
      "493   batch_normalization_131   False\n",
      "494   activation_128   False\n",
      "495   activation_131   False\n",
      "496   block17_14_mixed   False\n",
      "497   block17_14_conv   False\n",
      "498   block17_14   False\n",
      "499   block17_14_ac   False\n",
      "500   conv2d_133   False\n",
      "501   batch_normalization_133   False\n",
      "502   activation_133   False\n",
      "503   conv2d_134   False\n",
      "504   batch_normalization_134   False\n",
      "505   activation_134   False\n",
      "506   conv2d_132   False\n",
      "507   conv2d_135   False\n",
      "508   batch_normalization_132   False\n",
      "509   batch_normalization_135   False\n",
      "510   activation_132   False\n",
      "511   activation_135   False\n",
      "512   block17_15_mixed   False\n",
      "513   block17_15_conv   False\n",
      "514   block17_15   False\n",
      "515   block17_15_ac   False\n",
      "516   conv2d_137   False\n",
      "517   batch_normalization_137   False\n",
      "518   activation_137   False\n",
      "519   conv2d_138   False\n",
      "520   batch_normalization_138   False\n",
      "521   activation_138   False\n",
      "522   conv2d_136   False\n",
      "523   conv2d_139   False\n",
      "524   batch_normalization_136   False\n",
      "525   batch_normalization_139   False\n",
      "526   activation_136   False\n",
      "527   activation_139   False\n",
      "528   block17_16_mixed   False\n",
      "529   block17_16_conv   False\n",
      "530   block17_16   False\n",
      "531   block17_16_ac   False\n",
      "532   conv2d_141   False\n",
      "533   batch_normalization_141   False\n",
      "534   activation_141   False\n",
      "535   conv2d_142   False\n",
      "536   batch_normalization_142   False\n",
      "537   activation_142   False\n",
      "538   conv2d_140   False\n",
      "539   conv2d_143   False\n",
      "540   batch_normalization_140   False\n",
      "541   batch_normalization_143   False\n",
      "542   activation_140   False\n",
      "543   activation_143   False\n",
      "544   block17_17_mixed   False\n",
      "545   block17_17_conv   False\n",
      "546   block17_17   False\n",
      "547   block17_17_ac   False\n",
      "548   conv2d_145   False\n",
      "549   batch_normalization_145   False\n",
      "550   activation_145   False\n",
      "551   conv2d_146   False\n",
      "552   batch_normalization_146   False\n",
      "553   activation_146   False\n",
      "554   conv2d_144   False\n",
      "555   conv2d_147   False\n",
      "556   batch_normalization_144   False\n",
      "557   batch_normalization_147   False\n",
      "558   activation_144   False\n",
      "559   activation_147   False\n",
      "560   block17_18_mixed   False\n",
      "561   block17_18_conv   False\n",
      "562   block17_18   False\n",
      "563   block17_18_ac   False\n",
      "564   conv2d_149   False\n",
      "565   batch_normalization_149   False\n",
      "566   activation_149   False\n",
      "567   conv2d_150   False\n",
      "568   batch_normalization_150   False\n",
      "569   activation_150   False\n",
      "570   conv2d_148   False\n",
      "571   conv2d_151   False\n",
      "572   batch_normalization_148   False\n",
      "573   batch_normalization_151   False\n",
      "574   activation_148   False\n",
      "575   activation_151   False\n",
      "576   block17_19_mixed   False\n",
      "577   block17_19_conv   False\n",
      "578   block17_19   False\n",
      "579   block17_19_ac   False\n",
      "580   conv2d_153   False\n",
      "581   batch_normalization_153   False\n",
      "582   activation_153   False\n",
      "583   conv2d_154   False\n",
      "584   batch_normalization_154   False\n",
      "585   activation_154   False\n",
      "586   conv2d_152   False\n",
      "587   conv2d_155   False\n",
      "588   batch_normalization_152   False\n",
      "589   batch_normalization_155   False\n",
      "590   activation_152   False\n",
      "591   activation_155   False\n",
      "592   block17_20_mixed   False\n",
      "593   block17_20_conv   False\n",
      "594   block17_20   False\n",
      "595   block17_20_ac   False\n",
      "596   conv2d_160   False\n",
      "597   batch_normalization_160   False\n",
      "598   activation_160   False\n",
      "599   conv2d_156   False\n",
      "600   conv2d_158   False\n",
      "601   conv2d_161   False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602   batch_normalization_156   False\n",
      "603   batch_normalization_158   False\n",
      "604   batch_normalization_161   False\n",
      "605   activation_156   False\n",
      "606   activation_158   False\n",
      "607   activation_161   False\n",
      "608   conv2d_157   False\n",
      "609   conv2d_159   False\n",
      "610   conv2d_162   False\n",
      "611   batch_normalization_157   False\n",
      "612   batch_normalization_159   False\n",
      "613   batch_normalization_162   False\n",
      "614   activation_157   False\n",
      "615   activation_159   False\n",
      "616   activation_162   False\n",
      "617   max_pooling2d_3   False\n",
      "618   mixed_7a   False\n",
      "619   conv2d_164   False\n",
      "620   batch_normalization_164   False\n",
      "621   activation_164   False\n",
      "622   conv2d_165   False\n",
      "623   batch_normalization_165   False\n",
      "624   activation_165   False\n",
      "625   conv2d_163   False\n",
      "626   conv2d_166   False\n",
      "627   batch_normalization_163   False\n",
      "628   batch_normalization_166   False\n",
      "629   activation_163   False\n",
      "630   activation_166   False\n",
      "631   block8_1_mixed   False\n",
      "632   block8_1_conv   False\n",
      "633   block8_1   False\n",
      "634   block8_1_ac   False\n",
      "635   conv2d_168   False\n",
      "636   batch_normalization_168   False\n",
      "637   activation_168   False\n",
      "638   conv2d_169   False\n",
      "639   batch_normalization_169   False\n",
      "640   activation_169   False\n",
      "641   conv2d_167   False\n",
      "642   conv2d_170   False\n",
      "643   batch_normalization_167   False\n",
      "644   batch_normalization_170   False\n",
      "645   activation_167   False\n",
      "646   activation_170   False\n",
      "647   block8_2_mixed   False\n",
      "648   block8_2_conv   False\n",
      "649   block8_2   False\n",
      "650   block8_2_ac   False\n",
      "651   conv2d_172   False\n",
      "652   batch_normalization_172   False\n",
      "653   activation_172   False\n",
      "654   conv2d_173   False\n",
      "655   batch_normalization_173   False\n",
      "656   activation_173   False\n",
      "657   conv2d_171   False\n",
      "658   conv2d_174   False\n",
      "659   batch_normalization_171   False\n",
      "660   batch_normalization_174   False\n",
      "661   activation_171   False\n",
      "662   activation_174   False\n",
      "663   block8_3_mixed   False\n",
      "664   block8_3_conv   False\n",
      "665   block8_3   False\n",
      "666   block8_3_ac   False\n",
      "667   conv2d_176   False\n",
      "668   batch_normalization_176   False\n",
      "669   activation_176   False\n",
      "670   conv2d_177   False\n",
      "671   batch_normalization_177   False\n",
      "672   activation_177   False\n",
      "673   conv2d_175   False\n",
      "674   conv2d_178   False\n",
      "675   batch_normalization_175   False\n",
      "676   batch_normalization_178   False\n",
      "677   activation_175   False\n",
      "678   activation_178   False\n",
      "679   block8_4_mixed   False\n",
      "680   block8_4_conv   False\n",
      "681   block8_4   False\n",
      "682   block8_4_ac   False\n",
      "683   conv2d_180   False\n",
      "684   batch_normalization_180   False\n",
      "685   activation_180   False\n",
      "686   conv2d_181   False\n",
      "687   batch_normalization_181   False\n",
      "688   activation_181   False\n",
      "689   conv2d_179   False\n",
      "690   conv2d_182   False\n",
      "691   batch_normalization_179   False\n",
      "692   batch_normalization_182   False\n",
      "693   activation_179   False\n",
      "694   activation_182   False\n",
      "695   block8_5_mixed   False\n",
      "696   block8_5_conv   False\n",
      "697   block8_5   False\n",
      "698   block8_5_ac   False\n",
      "699   conv2d_184   False\n",
      "700   batch_normalization_184   False\n",
      "701   activation_184   False\n",
      "702   conv2d_185   False\n",
      "703   batch_normalization_185   False\n",
      "704   activation_185   False\n",
      "705   conv2d_183   False\n",
      "706   conv2d_186   False\n",
      "707   batch_normalization_183   False\n",
      "708   batch_normalization_186   False\n",
      "709   activation_183   False\n",
      "710   activation_186   False\n",
      "711   block8_6_mixed   False\n",
      "712   block8_6_conv   False\n",
      "713   block8_6   False\n",
      "714   block8_6_ac   False\n",
      "715   conv2d_188   False\n",
      "716   batch_normalization_188   False\n",
      "717   activation_188   False\n",
      "718   conv2d_189   False\n",
      "719   batch_normalization_189   False\n",
      "720   activation_189   False\n",
      "721   conv2d_187   False\n",
      "722   conv2d_190   False\n",
      "723   batch_normalization_187   False\n",
      "724   batch_normalization_190   False\n",
      "725   activation_187   False\n",
      "726   activation_190   False\n",
      "727   block8_7_mixed   False\n",
      "728   block8_7_conv   False\n",
      "729   block8_7   False\n",
      "730   block8_7_ac   False\n",
      "731   conv2d_192   False\n",
      "732   batch_normalization_192   False\n",
      "733   activation_192   False\n",
      "734   conv2d_193   False\n",
      "735   batch_normalization_193   False\n",
      "736   activation_193   False\n",
      "737   conv2d_191   False\n",
      "738   conv2d_194   False\n",
      "739   batch_normalization_191   False\n",
      "740   batch_normalization_194   False\n",
      "741   activation_191   False\n",
      "742   activation_194   False\n",
      "743   block8_8_mixed   False\n",
      "744   block8_8_conv   False\n",
      "745   block8_8   False\n",
      "746   block8_8_ac   False\n",
      "747   conv2d_196   False\n",
      "748   batch_normalization_196   False\n",
      "749   activation_196   False\n",
      "750   conv2d_197   False\n",
      "751   batch_normalization_197   False\n",
      "752   activation_197   False\n",
      "753   conv2d_195   False\n",
      "754   conv2d_198   False\n",
      "755   batch_normalization_195   False\n",
      "756   batch_normalization_198   False\n",
      "757   activation_195   False\n",
      "758   activation_198   False\n",
      "759   block8_9_mixed   False\n",
      "760   block8_9_conv   False\n",
      "761   block8_9   False\n",
      "762   block8_9_ac   False\n",
      "763   conv2d_200   False\n",
      "764   batch_normalization_200   False\n",
      "765   activation_200   False\n",
      "766   conv2d_201   False\n",
      "767   batch_normalization_201   False\n",
      "768   activation_201   False\n",
      "769   conv2d_199   False\n",
      "770   conv2d_202   False\n",
      "771   batch_normalization_199   False\n",
      "772   batch_normalization_202   False\n",
      "773   activation_199   False\n",
      "774   activation_202   False\n",
      "775   block8_10_mixed   False\n",
      "776   block8_10_conv   False\n",
      "777   block8_10   False\n",
      "778   conv_7b   False\n",
      "779   conv_7b_bn   False\n",
      "780   conv_7b_ac   True\n",
      "model retrieved\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_resnet_v2 (Functio (None, 5, 5, 1536)        54336736  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 38400)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              39322624  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 95,768,810\n",
      "Trainable params: 41,432,074\n",
      "Non-trainable params: 54,336,736\n",
      "_________________________________________________________________\n",
      "model building done\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 710s 9s/step - loss: 12.2378 - accuracy: 0.3624 - val_loss: 0.8519 - val_accuracy: 0.7484\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 704s 9s/step - loss: 0.8653 - accuracy: 0.7374 - val_loss: 0.8182 - val_accuracy: 0.7406\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 702s 9s/step - loss: 0.8141 - accuracy: 0.7426 - val_loss: 0.7939 - val_accuracy: 0.7578\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 703s 9s/step - loss: 0.7917 - accuracy: 0.7539 - val_loss: 0.7693 - val_accuracy: 0.7828\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 705s 9s/step - loss: 0.7337 - accuracy: 0.7665 - val_loss: 0.7814 - val_accuracy: 0.7719\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 706s 9s/step - loss: 0.6797 - accuracy: 0.7750 - val_loss: 0.8646 - val_accuracy: 0.7641\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 707s 9s/step - loss: 0.6561 - accuracy: 0.7921 - val_loss: 0.7766 - val_accuracy: 0.7672\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 704s 9s/step - loss: 0.5771 - accuracy: 0.8093 - val_loss: 0.7676 - val_accuracy: 0.7547\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 774s 10s/step - loss: 0.6067 - accuracy: 0.7945 - val_loss: 0.8471 - val_accuracy: 0.7641\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 821s 10s/step - loss: 0.6033 - accuracy: 0.7910 - val_loss: 0.8695 - val_accuracy: 0.7578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 275s 9s/step - loss: 0.8864 - accuracy: 0.7445\n",
      "model training done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 29753<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210413_051242-yik4f94z/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210413_051242-yik4f94z/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.58363</td></tr><tr><td>accuracy</td><td>0.8041</td></tr><tr><td>val_loss</td><td>0.86955</td></tr><tr><td>val_accuracy</td><td>0.75781</td></tr><tr><td>_runtime</td><td>7525</td></tr><tr><td>_timestamp</td><td>1618278487</td></tr><tr><td>_step</td><td>10</td></tr><tr><td>best_val_accuracy</td><td>0.78281</td></tr><tr><td>best_epoch</td><td>3</td></tr><tr><td>test_acc </td><td>0.7445</td></tr><tr><td>test_loss </td><td>0.88642</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>accuracy</td><td>▁▆▆▇▇▇████</td></tr><tr><td>val_loss</td><td>▇▄▃▁▂█▂▁▆█</td></tr><tr><td>val_accuracy</td><td>▂▁▄█▆▅▅▃▅▄</td></tr><tr><td>_runtime</td><td>▁▂▂▃▄▅▅▆▇██</td></tr><tr><td>_timestamp</td><td>▁▂▂▃▄▅▅▆▇██</td></tr><tr><td>_step</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>test_acc </td><td>▁</td></tr><tr><td>test_loss </td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">cs6910-a2-partB</strong>: <a href=\"https://wandb.ai/notarchana/cs6910-a2/runs/yik4f94z\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/runs/yik4f94z</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ivzgtuu0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: ['InceptionResNetV2', 742, 1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.25<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">cs6910-a2-partB</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/notarchana/cs6910-a2\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/notarchana/cs6910-a2/sweeps/wyy702q1\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/sweeps/wyy702q1</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/notarchana/cs6910-a2/runs/ivzgtuu0\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/runs/ivzgtuu0</a><br/>\n",
       "                Run data is saved locally in <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210413_071820-ivzgtuu0</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742\n",
      "1   input_1   False\n",
      "2   conv2d   False\n",
      "3   batch_normalization   False\n",
      "4   activation   False\n",
      "5   conv2d_1   False\n",
      "6   batch_normalization_1   False\n",
      "7   activation_1   False\n",
      "8   conv2d_2   False\n",
      "9   batch_normalization_2   False\n",
      "10   activation_2   False\n",
      "11   max_pooling2d   False\n",
      "12   conv2d_3   False\n",
      "13   batch_normalization_3   False\n",
      "14   activation_3   False\n",
      "15   conv2d_4   False\n",
      "16   batch_normalization_4   False\n",
      "17   activation_4   False\n",
      "18   max_pooling2d_1   False\n",
      "19   conv2d_8   False\n",
      "20   batch_normalization_8   False\n",
      "21   activation_8   False\n",
      "22   conv2d_6   False\n",
      "23   conv2d_9   False\n",
      "24   batch_normalization_6   False\n",
      "25   batch_normalization_9   False\n",
      "26   activation_6   False\n",
      "27   activation_9   False\n",
      "28   average_pooling2d   False\n",
      "29   conv2d_5   False\n",
      "30   conv2d_7   False\n",
      "31   conv2d_10   False\n",
      "32   conv2d_11   False\n",
      "33   batch_normalization_5   False\n",
      "34   batch_normalization_7   False\n",
      "35   batch_normalization_10   False\n",
      "36   batch_normalization_11   False\n",
      "37   activation_5   False\n",
      "38   activation_7   False\n",
      "39   activation_10   False\n",
      "40   activation_11   False\n",
      "41   mixed_5b   False\n",
      "42   conv2d_15   False\n",
      "43   batch_normalization_15   False\n",
      "44   activation_15   False\n",
      "45   conv2d_13   False\n",
      "46   conv2d_16   False\n",
      "47   batch_normalization_13   False\n",
      "48   batch_normalization_16   False\n",
      "49   activation_13   False\n",
      "50   activation_16   False\n",
      "51   conv2d_12   False\n",
      "52   conv2d_14   False\n",
      "53   conv2d_17   False\n",
      "54   batch_normalization_12   False\n",
      "55   batch_normalization_14   False\n",
      "56   batch_normalization_17   False\n",
      "57   activation_12   False\n",
      "58   activation_14   False\n",
      "59   activation_17   False\n",
      "60   block35_1_mixed   False\n",
      "61   block35_1_conv   False\n",
      "62   block35_1   False\n",
      "63   block35_1_ac   False\n",
      "64   conv2d_21   False\n",
      "65   batch_normalization_21   False\n",
      "66   activation_21   False\n",
      "67   conv2d_19   False\n",
      "68   conv2d_22   False\n",
      "69   batch_normalization_19   False\n",
      "70   batch_normalization_22   False\n",
      "71   activation_19   False\n",
      "72   activation_22   False\n",
      "73   conv2d_18   False\n",
      "74   conv2d_20   False\n",
      "75   conv2d_23   False\n",
      "76   batch_normalization_18   False\n",
      "77   batch_normalization_20   False\n",
      "78   batch_normalization_23   False\n",
      "79   activation_18   False\n",
      "80   activation_20   False\n",
      "81   activation_23   False\n",
      "82   block35_2_mixed   False\n",
      "83   block35_2_conv   False\n",
      "84   block35_2   False\n",
      "85   block35_2_ac   False\n",
      "86   conv2d_27   False\n",
      "87   batch_normalization_27   False\n",
      "88   activation_27   False\n",
      "89   conv2d_25   False\n",
      "90   conv2d_28   False\n",
      "91   batch_normalization_25   False\n",
      "92   batch_normalization_28   False\n",
      "93   activation_25   False\n",
      "94   activation_28   False\n",
      "95   conv2d_24   False\n",
      "96   conv2d_26   False\n",
      "97   conv2d_29   False\n",
      "98   batch_normalization_24   False\n",
      "99   batch_normalization_26   False\n",
      "100   batch_normalization_29   False\n",
      "101   activation_24   False\n",
      "102   activation_26   False\n",
      "103   activation_29   False\n",
      "104   block35_3_mixed   False\n",
      "105   block35_3_conv   False\n",
      "106   block35_3   False\n",
      "107   block35_3_ac   False\n",
      "108   conv2d_33   False\n",
      "109   batch_normalization_33   False\n",
      "110   activation_33   False\n",
      "111   conv2d_31   False\n",
      "112   conv2d_34   False\n",
      "113   batch_normalization_31   False\n",
      "114   batch_normalization_34   False\n",
      "115   activation_31   False\n",
      "116   activation_34   False\n",
      "117   conv2d_30   False\n",
      "118   conv2d_32   False\n",
      "119   conv2d_35   False\n",
      "120   batch_normalization_30   False\n",
      "121   batch_normalization_32   False\n",
      "122   batch_normalization_35   False\n",
      "123   activation_30   False\n",
      "124   activation_32   False\n",
      "125   activation_35   False\n",
      "126   block35_4_mixed   False\n",
      "127   block35_4_conv   False\n",
      "128   block35_4   False\n",
      "129   block35_4_ac   False\n",
      "130   conv2d_39   False\n",
      "131   batch_normalization_39   False\n",
      "132   activation_39   False\n",
      "133   conv2d_37   False\n",
      "134   conv2d_40   False\n",
      "135   batch_normalization_37   False\n",
      "136   batch_normalization_40   False\n",
      "137   activation_37   False\n",
      "138   activation_40   False\n",
      "139   conv2d_36   False\n",
      "140   conv2d_38   False\n",
      "141   conv2d_41   False\n",
      "142   batch_normalization_36   False\n",
      "143   batch_normalization_38   False\n",
      "144   batch_normalization_41   False\n",
      "145   activation_36   False\n",
      "146   activation_38   False\n",
      "147   activation_41   False\n",
      "148   block35_5_mixed   False\n",
      "149   block35_5_conv   False\n",
      "150   block35_5   False\n",
      "151   block35_5_ac   False\n",
      "152   conv2d_45   False\n",
      "153   batch_normalization_45   False\n",
      "154   activation_45   False\n",
      "155   conv2d_43   False\n",
      "156   conv2d_46   False\n",
      "157   batch_normalization_43   False\n",
      "158   batch_normalization_46   False\n",
      "159   activation_43   False\n",
      "160   activation_46   False\n",
      "161   conv2d_42   False\n",
      "162   conv2d_44   False\n",
      "163   conv2d_47   False\n",
      "164   batch_normalization_42   False\n",
      "165   batch_normalization_44   False\n",
      "166   batch_normalization_47   False\n",
      "167   activation_42   False\n",
      "168   activation_44   False\n",
      "169   activation_47   False\n",
      "170   block35_6_mixed   False\n",
      "171   block35_6_conv   False\n",
      "172   block35_6   False\n",
      "173   block35_6_ac   False\n",
      "174   conv2d_51   False\n",
      "175   batch_normalization_51   False\n",
      "176   activation_51   False\n",
      "177   conv2d_49   False\n",
      "178   conv2d_52   False\n",
      "179   batch_normalization_49   False\n",
      "180   batch_normalization_52   False\n",
      "181   activation_49   False\n",
      "182   activation_52   False\n",
      "183   conv2d_48   False\n",
      "184   conv2d_50   False\n",
      "185   conv2d_53   False\n",
      "186   batch_normalization_48   False\n",
      "187   batch_normalization_50   False\n",
      "188   batch_normalization_53   False\n",
      "189   activation_48   False\n",
      "190   activation_50   False\n",
      "191   activation_53   False\n",
      "192   block35_7_mixed   False\n",
      "193   block35_7_conv   False\n",
      "194   block35_7   False\n",
      "195   block35_7_ac   False\n",
      "196   conv2d_57   False\n",
      "197   batch_normalization_57   False\n",
      "198   activation_57   False\n",
      "199   conv2d_55   False\n",
      "200   conv2d_58   False\n",
      "201   batch_normalization_55   False\n",
      "202   batch_normalization_58   False\n",
      "203   activation_55   False\n",
      "204   activation_58   False\n",
      "205   conv2d_54   False\n",
      "206   conv2d_56   False\n",
      "207   conv2d_59   False\n",
      "208   batch_normalization_54   False\n",
      "209   batch_normalization_56   False\n",
      "210   batch_normalization_59   False\n",
      "211   activation_54   False\n",
      "212   activation_56   False\n",
      "213   activation_59   False\n",
      "214   block35_8_mixed   False\n",
      "215   block35_8_conv   False\n",
      "216   block35_8   False\n",
      "217   block35_8_ac   False\n",
      "218   conv2d_63   False\n",
      "219   batch_normalization_63   False\n",
      "220   activation_63   False\n",
      "221   conv2d_61   False\n",
      "222   conv2d_64   False\n",
      "223   batch_normalization_61   False\n",
      "224   batch_normalization_64   False\n",
      "225   activation_61   False\n",
      "226   activation_64   False\n",
      "227   conv2d_60   False\n",
      "228   conv2d_62   False\n",
      "229   conv2d_65   False\n",
      "230   batch_normalization_60   False\n",
      "231   batch_normalization_62   False\n",
      "232   batch_normalization_65   False\n",
      "233   activation_60   False\n",
      "234   activation_62   False\n",
      "235   activation_65   False\n",
      "236   block35_9_mixed   False\n",
      "237   block35_9_conv   False\n",
      "238   block35_9   False\n",
      "239   block35_9_ac   False\n",
      "240   conv2d_69   False\n",
      "241   batch_normalization_69   False\n",
      "242   activation_69   False\n",
      "243   conv2d_67   False\n",
      "244   conv2d_70   False\n",
      "245   batch_normalization_67   False\n",
      "246   batch_normalization_70   False\n",
      "247   activation_67   False\n",
      "248   activation_70   False\n",
      "249   conv2d_66   False\n",
      "250   conv2d_68   False\n",
      "251   conv2d_71   False\n",
      "252   batch_normalization_66   False\n",
      "253   batch_normalization_68   False\n",
      "254   batch_normalization_71   False\n",
      "255   activation_66   False\n",
      "256   activation_68   False\n",
      "257   activation_71   False\n",
      "258   block35_10_mixed   False\n",
      "259   block35_10_conv   False\n",
      "260   block35_10   False\n",
      "261   block35_10_ac   False\n",
      "262   conv2d_73   False\n",
      "263   batch_normalization_73   False\n",
      "264   activation_73   False\n",
      "265   conv2d_74   False\n",
      "266   batch_normalization_74   False\n",
      "267   activation_74   False\n",
      "268   conv2d_72   False\n",
      "269   conv2d_75   False\n",
      "270   batch_normalization_72   False\n",
      "271   batch_normalization_75   False\n",
      "272   activation_72   False\n",
      "273   activation_75   False\n",
      "274   max_pooling2d_2   False\n",
      "275   mixed_6a   False\n",
      "276   conv2d_77   False\n",
      "277   batch_normalization_77   False\n",
      "278   activation_77   False\n",
      "279   conv2d_78   False\n",
      "280   batch_normalization_78   False\n",
      "281   activation_78   False\n",
      "282   conv2d_76   False\n",
      "283   conv2d_79   False\n",
      "284   batch_normalization_76   False\n",
      "285   batch_normalization_79   False\n",
      "286   activation_76   False\n",
      "287   activation_79   False\n",
      "288   block17_1_mixed   False\n",
      "289   block17_1_conv   False\n",
      "290   block17_1   False\n",
      "291   block17_1_ac   False\n",
      "292   conv2d_81   False\n",
      "293   batch_normalization_81   False\n",
      "294   activation_81   False\n",
      "295   conv2d_82   False\n",
      "296   batch_normalization_82   False\n",
      "297   activation_82   False\n",
      "298   conv2d_80   False\n",
      "299   conv2d_83   False\n",
      "300   batch_normalization_80   False\n",
      "301   batch_normalization_83   False\n",
      "302   activation_80   False\n",
      "303   activation_83   False\n",
      "304   block17_2_mixed   False\n",
      "305   block17_2_conv   False\n",
      "306   block17_2   False\n",
      "307   block17_2_ac   False\n",
      "308   conv2d_85   False\n",
      "309   batch_normalization_85   False\n",
      "310   activation_85   False\n",
      "311   conv2d_86   False\n",
      "312   batch_normalization_86   False\n",
      "313   activation_86   False\n",
      "314   conv2d_84   False\n",
      "315   conv2d_87   False\n",
      "316   batch_normalization_84   False\n",
      "317   batch_normalization_87   False\n",
      "318   activation_84   False\n",
      "319   activation_87   False\n",
      "320   block17_3_mixed   False\n",
      "321   block17_3_conv   False\n",
      "322   block17_3   False\n",
      "323   block17_3_ac   False\n",
      "324   conv2d_89   False\n",
      "325   batch_normalization_89   False\n",
      "326   activation_89   False\n",
      "327   conv2d_90   False\n",
      "328   batch_normalization_90   False\n",
      "329   activation_90   False\n",
      "330   conv2d_88   False\n",
      "331   conv2d_91   False\n",
      "332   batch_normalization_88   False\n",
      "333   batch_normalization_91   False\n",
      "334   activation_88   False\n",
      "335   activation_91   False\n",
      "336   block17_4_mixed   False\n",
      "337   block17_4_conv   False\n",
      "338   block17_4   False\n",
      "339   block17_4_ac   False\n",
      "340   conv2d_93   False\n",
      "341   batch_normalization_93   False\n",
      "342   activation_93   False\n",
      "343   conv2d_94   False\n",
      "344   batch_normalization_94   False\n",
      "345   activation_94   False\n",
      "346   conv2d_92   False\n",
      "347   conv2d_95   False\n",
      "348   batch_normalization_92   False\n",
      "349   batch_normalization_95   False\n",
      "350   activation_92   False\n",
      "351   activation_95   False\n",
      "352   block17_5_mixed   False\n",
      "353   block17_5_conv   False\n",
      "354   block17_5   False\n",
      "355   block17_5_ac   False\n",
      "356   conv2d_97   False\n",
      "357   batch_normalization_97   False\n",
      "358   activation_97   False\n",
      "359   conv2d_98   False\n",
      "360   batch_normalization_98   False\n",
      "361   activation_98   False\n",
      "362   conv2d_96   False\n",
      "363   conv2d_99   False\n",
      "364   batch_normalization_96   False\n",
      "365   batch_normalization_99   False\n",
      "366   activation_96   False\n",
      "367   activation_99   False\n",
      "368   block17_6_mixed   False\n",
      "369   block17_6_conv   False\n",
      "370   block17_6   False\n",
      "371   block17_6_ac   False\n",
      "372   conv2d_101   False\n",
      "373   batch_normalization_101   False\n",
      "374   activation_101   False\n",
      "375   conv2d_102   False\n",
      "376   batch_normalization_102   False\n",
      "377   activation_102   False\n",
      "378   conv2d_100   False\n",
      "379   conv2d_103   False\n",
      "380   batch_normalization_100   False\n",
      "381   batch_normalization_103   False\n",
      "382   activation_100   False\n",
      "383   activation_103   False\n",
      "384   block17_7_mixed   False\n",
      "385   block17_7_conv   False\n",
      "386   block17_7   False\n",
      "387   block17_7_ac   False\n",
      "388   conv2d_105   False\n",
      "389   batch_normalization_105   False\n",
      "390   activation_105   False\n",
      "391   conv2d_106   False\n",
      "392   batch_normalization_106   False\n",
      "393   activation_106   False\n",
      "394   conv2d_104   False\n",
      "395   conv2d_107   False\n",
      "396   batch_normalization_104   False\n",
      "397   batch_normalization_107   False\n",
      "398   activation_104   False\n",
      "399   activation_107   False\n",
      "400   block17_8_mixed   False\n",
      "401   block17_8_conv   False\n",
      "402   block17_8   False\n",
      "403   block17_8_ac   False\n",
      "404   conv2d_109   False\n",
      "405   batch_normalization_109   False\n",
      "406   activation_109   False\n",
      "407   conv2d_110   False\n",
      "408   batch_normalization_110   False\n",
      "409   activation_110   False\n",
      "410   conv2d_108   False\n",
      "411   conv2d_111   False\n",
      "412   batch_normalization_108   False\n",
      "413   batch_normalization_111   False\n",
      "414   activation_108   False\n",
      "415   activation_111   False\n",
      "416   block17_9_mixed   False\n",
      "417   block17_9_conv   False\n",
      "418   block17_9   False\n",
      "419   block17_9_ac   False\n",
      "420   conv2d_113   False\n",
      "421   batch_normalization_113   False\n",
      "422   activation_113   False\n",
      "423   conv2d_114   False\n",
      "424   batch_normalization_114   False\n",
      "425   activation_114   False\n",
      "426   conv2d_112   False\n",
      "427   conv2d_115   False\n",
      "428   batch_normalization_112   False\n",
      "429   batch_normalization_115   False\n",
      "430   activation_112   False\n",
      "431   activation_115   False\n",
      "432   block17_10_mixed   False\n",
      "433   block17_10_conv   False\n",
      "434   block17_10   False\n",
      "435   block17_10_ac   False\n",
      "436   conv2d_117   False\n",
      "437   batch_normalization_117   False\n",
      "438   activation_117   False\n",
      "439   conv2d_118   False\n",
      "440   batch_normalization_118   False\n",
      "441   activation_118   False\n",
      "442   conv2d_116   False\n",
      "443   conv2d_119   False\n",
      "444   batch_normalization_116   False\n",
      "445   batch_normalization_119   False\n",
      "446   activation_116   False\n",
      "447   activation_119   False\n",
      "448   block17_11_mixed   False\n",
      "449   block17_11_conv   False\n",
      "450   block17_11   False\n",
      "451   block17_11_ac   False\n",
      "452   conv2d_121   False\n",
      "453   batch_normalization_121   False\n",
      "454   activation_121   False\n",
      "455   conv2d_122   False\n",
      "456   batch_normalization_122   False\n",
      "457   activation_122   False\n",
      "458   conv2d_120   False\n",
      "459   conv2d_123   False\n",
      "460   batch_normalization_120   False\n",
      "461   batch_normalization_123   False\n",
      "462   activation_120   False\n",
      "463   activation_123   False\n",
      "464   block17_12_mixed   False\n",
      "465   block17_12_conv   False\n",
      "466   block17_12   False\n",
      "467   block17_12_ac   False\n",
      "468   conv2d_125   False\n",
      "469   batch_normalization_125   False\n",
      "470   activation_125   False\n",
      "471   conv2d_126   False\n",
      "472   batch_normalization_126   False\n",
      "473   activation_126   False\n",
      "474   conv2d_124   False\n",
      "475   conv2d_127   False\n",
      "476   batch_normalization_124   False\n",
      "477   batch_normalization_127   False\n",
      "478   activation_124   False\n",
      "479   activation_127   False\n",
      "480   block17_13_mixed   False\n",
      "481   block17_13_conv   False\n",
      "482   block17_13   False\n",
      "483   block17_13_ac   False\n",
      "484   conv2d_129   False\n",
      "485   batch_normalization_129   False\n",
      "486   activation_129   False\n",
      "487   conv2d_130   False\n",
      "488   batch_normalization_130   False\n",
      "489   activation_130   False\n",
      "490   conv2d_128   False\n",
      "491   conv2d_131   False\n",
      "492   batch_normalization_128   False\n",
      "493   batch_normalization_131   False\n",
      "494   activation_128   False\n",
      "495   activation_131   False\n",
      "496   block17_14_mixed   False\n",
      "497   block17_14_conv   False\n",
      "498   block17_14   False\n",
      "499   block17_14_ac   False\n",
      "500   conv2d_133   False\n",
      "501   batch_normalization_133   False\n",
      "502   activation_133   False\n",
      "503   conv2d_134   False\n",
      "504   batch_normalization_134   False\n",
      "505   activation_134   False\n",
      "506   conv2d_132   False\n",
      "507   conv2d_135   False\n",
      "508   batch_normalization_132   False\n",
      "509   batch_normalization_135   False\n",
      "510   activation_132   False\n",
      "511   activation_135   False\n",
      "512   block17_15_mixed   False\n",
      "513   block17_15_conv   False\n",
      "514   block17_15   False\n",
      "515   block17_15_ac   False\n",
      "516   conv2d_137   False\n",
      "517   batch_normalization_137   False\n",
      "518   activation_137   False\n",
      "519   conv2d_138   False\n",
      "520   batch_normalization_138   False\n",
      "521   activation_138   False\n",
      "522   conv2d_136   False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523   conv2d_139   False\n",
      "524   batch_normalization_136   False\n",
      "525   batch_normalization_139   False\n",
      "526   activation_136   False\n",
      "527   activation_139   False\n",
      "528   block17_16_mixed   False\n",
      "529   block17_16_conv   False\n",
      "530   block17_16   False\n",
      "531   block17_16_ac   False\n",
      "532   conv2d_141   False\n",
      "533   batch_normalization_141   False\n",
      "534   activation_141   False\n",
      "535   conv2d_142   False\n",
      "536   batch_normalization_142   False\n",
      "537   activation_142   False\n",
      "538   conv2d_140   False\n",
      "539   conv2d_143   False\n",
      "540   batch_normalization_140   False\n",
      "541   batch_normalization_143   False\n",
      "542   activation_140   False\n",
      "543   activation_143   False\n",
      "544   block17_17_mixed   False\n",
      "545   block17_17_conv   False\n",
      "546   block17_17   False\n",
      "547   block17_17_ac   False\n",
      "548   conv2d_145   False\n",
      "549   batch_normalization_145   False\n",
      "550   activation_145   False\n",
      "551   conv2d_146   False\n",
      "552   batch_normalization_146   False\n",
      "553   activation_146   False\n",
      "554   conv2d_144   False\n",
      "555   conv2d_147   False\n",
      "556   batch_normalization_144   False\n",
      "557   batch_normalization_147   False\n",
      "558   activation_144   False\n",
      "559   activation_147   False\n",
      "560   block17_18_mixed   False\n",
      "561   block17_18_conv   False\n",
      "562   block17_18   False\n",
      "563   block17_18_ac   False\n",
      "564   conv2d_149   False\n",
      "565   batch_normalization_149   False\n",
      "566   activation_149   False\n",
      "567   conv2d_150   False\n",
      "568   batch_normalization_150   False\n",
      "569   activation_150   False\n",
      "570   conv2d_148   False\n",
      "571   conv2d_151   False\n",
      "572   batch_normalization_148   False\n",
      "573   batch_normalization_151   False\n",
      "574   activation_148   False\n",
      "575   activation_151   False\n",
      "576   block17_19_mixed   False\n",
      "577   block17_19_conv   False\n",
      "578   block17_19   False\n",
      "579   block17_19_ac   False\n",
      "580   conv2d_153   False\n",
      "581   batch_normalization_153   False\n",
      "582   activation_153   False\n",
      "583   conv2d_154   False\n",
      "584   batch_normalization_154   False\n",
      "585   activation_154   False\n",
      "586   conv2d_152   False\n",
      "587   conv2d_155   False\n",
      "588   batch_normalization_152   False\n",
      "589   batch_normalization_155   False\n",
      "590   activation_152   False\n",
      "591   activation_155   False\n",
      "592   block17_20_mixed   False\n",
      "593   block17_20_conv   False\n",
      "594   block17_20   False\n",
      "595   block17_20_ac   False\n",
      "596   conv2d_160   False\n",
      "597   batch_normalization_160   False\n",
      "598   activation_160   False\n",
      "599   conv2d_156   False\n",
      "600   conv2d_158   False\n",
      "601   conv2d_161   False\n",
      "602   batch_normalization_156   False\n",
      "603   batch_normalization_158   False\n",
      "604   batch_normalization_161   False\n",
      "605   activation_156   False\n",
      "606   activation_158   False\n",
      "607   activation_161   False\n",
      "608   conv2d_157   False\n",
      "609   conv2d_159   False\n",
      "610   conv2d_162   False\n",
      "611   batch_normalization_157   False\n",
      "612   batch_normalization_159   False\n",
      "613   batch_normalization_162   False\n",
      "614   activation_157   False\n",
      "615   activation_159   False\n",
      "616   activation_162   False\n",
      "617   max_pooling2d_3   False\n",
      "618   mixed_7a   False\n",
      "619   conv2d_164   False\n",
      "620   batch_normalization_164   False\n",
      "621   activation_164   False\n",
      "622   conv2d_165   False\n",
      "623   batch_normalization_165   False\n",
      "624   activation_165   False\n",
      "625   conv2d_163   False\n",
      "626   conv2d_166   False\n",
      "627   batch_normalization_163   False\n",
      "628   batch_normalization_166   False\n",
      "629   activation_163   False\n",
      "630   activation_166   False\n",
      "631   block8_1_mixed   False\n",
      "632   block8_1_conv   False\n",
      "633   block8_1   False\n",
      "634   block8_1_ac   False\n",
      "635   conv2d_168   False\n",
      "636   batch_normalization_168   False\n",
      "637   activation_168   False\n",
      "638   conv2d_169   False\n",
      "639   batch_normalization_169   False\n",
      "640   activation_169   False\n",
      "641   conv2d_167   False\n",
      "642   conv2d_170   False\n",
      "643   batch_normalization_167   False\n",
      "644   batch_normalization_170   False\n",
      "645   activation_167   False\n",
      "646   activation_170   False\n",
      "647   block8_2_mixed   False\n",
      "648   block8_2_conv   False\n",
      "649   block8_2   False\n",
      "650   block8_2_ac   False\n",
      "651   conv2d_172   False\n",
      "652   batch_normalization_172   False\n",
      "653   activation_172   False\n",
      "654   conv2d_173   False\n",
      "655   batch_normalization_173   False\n",
      "656   activation_173   False\n",
      "657   conv2d_171   False\n",
      "658   conv2d_174   False\n",
      "659   batch_normalization_171   False\n",
      "660   batch_normalization_174   False\n",
      "661   activation_171   False\n",
      "662   activation_174   False\n",
      "663   block8_3_mixed   False\n",
      "664   block8_3_conv   False\n",
      "665   block8_3   False\n",
      "666   block8_3_ac   False\n",
      "667   conv2d_176   False\n",
      "668   batch_normalization_176   False\n",
      "669   activation_176   False\n",
      "670   conv2d_177   False\n",
      "671   batch_normalization_177   False\n",
      "672   activation_177   False\n",
      "673   conv2d_175   False\n",
      "674   conv2d_178   False\n",
      "675   batch_normalization_175   False\n",
      "676   batch_normalization_178   False\n",
      "677   activation_175   False\n",
      "678   activation_178   False\n",
      "679   block8_4_mixed   False\n",
      "680   block8_4_conv   False\n",
      "681   block8_4   False\n",
      "682   block8_4_ac   False\n",
      "683   conv2d_180   False\n",
      "684   batch_normalization_180   False\n",
      "685   activation_180   False\n",
      "686   conv2d_181   False\n",
      "687   batch_normalization_181   False\n",
      "688   activation_181   False\n",
      "689   conv2d_179   False\n",
      "690   conv2d_182   False\n",
      "691   batch_normalization_179   False\n",
      "692   batch_normalization_182   False\n",
      "693   activation_179   False\n",
      "694   activation_182   False\n",
      "695   block8_5_mixed   False\n",
      "696   block8_5_conv   False\n",
      "697   block8_5   False\n",
      "698   block8_5_ac   False\n",
      "699   conv2d_184   False\n",
      "700   batch_normalization_184   False\n",
      "701   activation_184   False\n",
      "702   conv2d_185   False\n",
      "703   batch_normalization_185   False\n",
      "704   activation_185   False\n",
      "705   conv2d_183   False\n",
      "706   conv2d_186   False\n",
      "707   batch_normalization_183   False\n",
      "708   batch_normalization_186   False\n",
      "709   activation_183   False\n",
      "710   activation_186   False\n",
      "711   block8_6_mixed   False\n",
      "712   block8_6_conv   False\n",
      "713   block8_6   False\n",
      "714   block8_6_ac   False\n",
      "715   conv2d_188   False\n",
      "716   batch_normalization_188   False\n",
      "717   activation_188   False\n",
      "718   conv2d_189   False\n",
      "719   batch_normalization_189   False\n",
      "720   activation_189   False\n",
      "721   conv2d_187   False\n",
      "722   conv2d_190   False\n",
      "723   batch_normalization_187   False\n",
      "724   batch_normalization_190   False\n",
      "725   activation_187   False\n",
      "726   activation_190   False\n",
      "727   block8_7_mixed   False\n",
      "728   block8_7_conv   False\n",
      "729   block8_7   False\n",
      "730   block8_7_ac   False\n",
      "731   conv2d_192   False\n",
      "732   batch_normalization_192   False\n",
      "733   activation_192   False\n",
      "734   conv2d_193   False\n",
      "735   batch_normalization_193   False\n",
      "736   activation_193   False\n",
      "737   conv2d_191   False\n",
      "738   conv2d_194   False\n",
      "739   batch_normalization_191   False\n",
      "740   batch_normalization_194   False\n",
      "741   activation_191   False\n",
      "742   activation_194   False\n",
      "743   block8_8_mixed   True\n",
      "744   block8_8_conv   True\n",
      "745   block8_8   True\n",
      "746   block8_8_ac   True\n",
      "747   conv2d_196   True\n",
      "748   batch_normalization_196   True\n",
      "749   activation_196   True\n",
      "750   conv2d_197   True\n",
      "751   batch_normalization_197   True\n",
      "752   activation_197   True\n",
      "753   conv2d_195   True\n",
      "754   conv2d_198   True\n",
      "755   batch_normalization_195   True\n",
      "756   batch_normalization_198   True\n",
      "757   activation_195   True\n",
      "758   activation_198   True\n",
      "759   block8_9_mixed   True\n",
      "760   block8_9_conv   True\n",
      "761   block8_9   True\n",
      "762   block8_9_ac   True\n",
      "763   conv2d_200   True\n",
      "764   batch_normalization_200   True\n",
      "765   activation_200   True\n",
      "766   conv2d_201   True\n",
      "767   batch_normalization_201   True\n",
      "768   activation_201   True\n",
      "769   conv2d_199   True\n",
      "770   conv2d_202   True\n",
      "771   batch_normalization_199   True\n",
      "772   batch_normalization_202   True\n",
      "773   activation_199   True\n",
      "774   activation_202   True\n",
      "775   block8_10_mixed   True\n",
      "776   block8_10_conv   True\n",
      "777   block8_10   True\n",
      "778   conv_7b   True\n",
      "779   conv_7b_bn   True\n",
      "780   conv_7b_ac   True\n",
      "model retrieved\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_resnet_v2 (Functio (None, 5, 5, 1536)        54336736  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 38400)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              39322624  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 95,768,810\n",
      "Trainable params: 49,631,530\n",
      "Non-trainable params: 46,137,280\n",
      "_________________________________________________________________\n",
      "model building done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "80/80 [==============================] - 779s 10s/step - loss: 6.1783 - accuracy: 0.3079 - val_loss: 1.5776 - val_accuracy: 0.6219\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 828s 10s/step - loss: 0.9480 - accuracy: 0.7000 - val_loss: 0.9328 - val_accuracy: 0.7437\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 783s 10s/step - loss: 0.7507 - accuracy: 0.7673 - val_loss: 0.7661 - val_accuracy: 0.7734\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 749s 9s/step - loss: 0.7320 - accuracy: 0.7742 - val_loss: 0.7277 - val_accuracy: 0.7922\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 758s 9s/step - loss: 0.6889 - accuracy: 0.7848 - val_loss: 0.8302 - val_accuracy: 0.7672\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 752s 9s/step - loss: 0.6256 - accuracy: 0.8058 - val_loss: 0.7790 - val_accuracy: 0.7703\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 847s 11s/step - loss: 0.5905 - accuracy: 0.8144 - val_loss: 0.8325 - val_accuracy: 0.7688\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 978s 12s/step - loss: 0.5757 - accuracy: 0.8294 - val_loss: 0.9103 - val_accuracy: 0.7250\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 1079s 13s/step - loss: 0.5613 - accuracy: 0.8348 - val_loss: 0.7302 - val_accuracy: 0.7875\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 793s 10s/step - loss: 0.4877 - accuracy: 0.8537 - val_loss: 0.7862 - val_accuracy: 0.7719\n",
      "32/32 [==============================] - 250s 8s/step - loss: 0.7373 - accuracy: 0.7820\n",
      "model training done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 31180<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210413_071820-ivzgtuu0/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210413_071820-ivzgtuu0/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.50288</td></tr><tr><td>accuracy</td><td>0.84801</td></tr><tr><td>val_loss</td><td>0.78624</td></tr><tr><td>val_accuracy</td><td>0.77188</td></tr><tr><td>_runtime</td><td>8614</td></tr><tr><td>_timestamp</td><td>1618287115</td></tr><tr><td>_step</td><td>10</td></tr><tr><td>best_val_accuracy</td><td>0.79219</td></tr><tr><td>best_epoch</td><td>3</td></tr><tr><td>test_acc </td><td>0.782</td></tr><tr><td>test_loss </td><td>0.73725</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▂▂▂▁▁▁▁▁▁</td></tr><tr><td>accuracy</td><td>▁▆▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▁▁▂▁▂▃▁▁</td></tr><tr><td>val_accuracy</td><td>▁▆▇█▇▇▇▅█▇</td></tr><tr><td>_runtime</td><td>▁▂▂▃▄▄▅▆▇██</td></tr><tr><td>_timestamp</td><td>▁▂▂▃▄▄▅▆▇██</td></tr><tr><td>_step</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>test_acc </td><td>▁</td></tr><tr><td>test_loss </td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">cs6910-a2-partB</strong>: <a href=\"https://wandb.ai/notarchana/cs6910-a2/runs/ivzgtuu0\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/runs/ivzgtuu0</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: oicnhuuu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: ['InceptionResNetV2', 710, 2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.25<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">cs6910-a2-partB</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/notarchana/cs6910-a2\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/notarchana/cs6910-a2/sweeps/wyy702q1\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/sweeps/wyy702q1</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/notarchana/cs6910-a2/runs/oicnhuuu\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/runs/oicnhuuu</a><br/>\n",
       "                Run data is saved locally in <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210413_094342-oicnhuuu</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "710\n",
      "1   input_1   False\n",
      "2   conv2d   False\n",
      "3   batch_normalization   False\n",
      "4   activation   False\n",
      "5   conv2d_1   False\n",
      "6   batch_normalization_1   False\n",
      "7   activation_1   False\n",
      "8   conv2d_2   False\n",
      "9   batch_normalization_2   False\n",
      "10   activation_2   False\n",
      "11   max_pooling2d   False\n",
      "12   conv2d_3   False\n",
      "13   batch_normalization_3   False\n",
      "14   activation_3   False\n",
      "15   conv2d_4   False\n",
      "16   batch_normalization_4   False\n",
      "17   activation_4   False\n",
      "18   max_pooling2d_1   False\n",
      "19   conv2d_8   False\n",
      "20   batch_normalization_8   False\n",
      "21   activation_8   False\n",
      "22   conv2d_6   False\n",
      "23   conv2d_9   False\n",
      "24   batch_normalization_6   False\n",
      "25   batch_normalization_9   False\n",
      "26   activation_6   False\n",
      "27   activation_9   False\n",
      "28   average_pooling2d   False\n",
      "29   conv2d_5   False\n",
      "30   conv2d_7   False\n",
      "31   conv2d_10   False\n",
      "32   conv2d_11   False\n",
      "33   batch_normalization_5   False\n",
      "34   batch_normalization_7   False\n",
      "35   batch_normalization_10   False\n",
      "36   batch_normalization_11   False\n",
      "37   activation_5   False\n",
      "38   activation_7   False\n",
      "39   activation_10   False\n",
      "40   activation_11   False\n",
      "41   mixed_5b   False\n",
      "42   conv2d_15   False\n",
      "43   batch_normalization_15   False\n",
      "44   activation_15   False\n",
      "45   conv2d_13   False\n",
      "46   conv2d_16   False\n",
      "47   batch_normalization_13   False\n",
      "48   batch_normalization_16   False\n",
      "49   activation_13   False\n",
      "50   activation_16   False\n",
      "51   conv2d_12   False\n",
      "52   conv2d_14   False\n",
      "53   conv2d_17   False\n",
      "54   batch_normalization_12   False\n",
      "55   batch_normalization_14   False\n",
      "56   batch_normalization_17   False\n",
      "57   activation_12   False\n",
      "58   activation_14   False\n",
      "59   activation_17   False\n",
      "60   block35_1_mixed   False\n",
      "61   block35_1_conv   False\n",
      "62   block35_1   False\n",
      "63   block35_1_ac   False\n",
      "64   conv2d_21   False\n",
      "65   batch_normalization_21   False\n",
      "66   activation_21   False\n",
      "67   conv2d_19   False\n",
      "68   conv2d_22   False\n",
      "69   batch_normalization_19   False\n",
      "70   batch_normalization_22   False\n",
      "71   activation_19   False\n",
      "72   activation_22   False\n",
      "73   conv2d_18   False\n",
      "74   conv2d_20   False\n",
      "75   conv2d_23   False\n",
      "76   batch_normalization_18   False\n",
      "77   batch_normalization_20   False\n",
      "78   batch_normalization_23   False\n",
      "79   activation_18   False\n",
      "80   activation_20   False\n",
      "81   activation_23   False\n",
      "82   block35_2_mixed   False\n",
      "83   block35_2_conv   False\n",
      "84   block35_2   False\n",
      "85   block35_2_ac   False\n",
      "86   conv2d_27   False\n",
      "87   batch_normalization_27   False\n",
      "88   activation_27   False\n",
      "89   conv2d_25   False\n",
      "90   conv2d_28   False\n",
      "91   batch_normalization_25   False\n",
      "92   batch_normalization_28   False\n",
      "93   activation_25   False\n",
      "94   activation_28   False\n",
      "95   conv2d_24   False\n",
      "96   conv2d_26   False\n",
      "97   conv2d_29   False\n",
      "98   batch_normalization_24   False\n",
      "99   batch_normalization_26   False\n",
      "100   batch_normalization_29   False\n",
      "101   activation_24   False\n",
      "102   activation_26   False\n",
      "103   activation_29   False\n",
      "104   block35_3_mixed   False\n",
      "105   block35_3_conv   False\n",
      "106   block35_3   False\n",
      "107   block35_3_ac   False\n",
      "108   conv2d_33   False\n",
      "109   batch_normalization_33   False\n",
      "110   activation_33   False\n",
      "111   conv2d_31   False\n",
      "112   conv2d_34   False\n",
      "113   batch_normalization_31   False\n",
      "114   batch_normalization_34   False\n",
      "115   activation_31   False\n",
      "116   activation_34   False\n",
      "117   conv2d_30   False\n",
      "118   conv2d_32   False\n",
      "119   conv2d_35   False\n",
      "120   batch_normalization_30   False\n",
      "121   batch_normalization_32   False\n",
      "122   batch_normalization_35   False\n",
      "123   activation_30   False\n",
      "124   activation_32   False\n",
      "125   activation_35   False\n",
      "126   block35_4_mixed   False\n",
      "127   block35_4_conv   False\n",
      "128   block35_4   False\n",
      "129   block35_4_ac   False\n",
      "130   conv2d_39   False\n",
      "131   batch_normalization_39   False\n",
      "132   activation_39   False\n",
      "133   conv2d_37   False\n",
      "134   conv2d_40   False\n",
      "135   batch_normalization_37   False\n",
      "136   batch_normalization_40   False\n",
      "137   activation_37   False\n",
      "138   activation_40   False\n",
      "139   conv2d_36   False\n",
      "140   conv2d_38   False\n",
      "141   conv2d_41   False\n",
      "142   batch_normalization_36   False\n",
      "143   batch_normalization_38   False\n",
      "144   batch_normalization_41   False\n",
      "145   activation_36   False\n",
      "146   activation_38   False\n",
      "147   activation_41   False\n",
      "148   block35_5_mixed   False\n",
      "149   block35_5_conv   False\n",
      "150   block35_5   False\n",
      "151   block35_5_ac   False\n",
      "152   conv2d_45   False\n",
      "153   batch_normalization_45   False\n",
      "154   activation_45   False\n",
      "155   conv2d_43   False\n",
      "156   conv2d_46   False\n",
      "157   batch_normalization_43   False\n",
      "158   batch_normalization_46   False\n",
      "159   activation_43   False\n",
      "160   activation_46   False\n",
      "161   conv2d_42   False\n",
      "162   conv2d_44   False\n",
      "163   conv2d_47   False\n",
      "164   batch_normalization_42   False\n",
      "165   batch_normalization_44   False\n",
      "166   batch_normalization_47   False\n",
      "167   activation_42   False\n",
      "168   activation_44   False\n",
      "169   activation_47   False\n",
      "170   block35_6_mixed   False\n",
      "171   block35_6_conv   False\n",
      "172   block35_6   False\n",
      "173   block35_6_ac   False\n",
      "174   conv2d_51   False\n",
      "175   batch_normalization_51   False\n",
      "176   activation_51   False\n",
      "177   conv2d_49   False\n",
      "178   conv2d_52   False\n",
      "179   batch_normalization_49   False\n",
      "180   batch_normalization_52   False\n",
      "181   activation_49   False\n",
      "182   activation_52   False\n",
      "183   conv2d_48   False\n",
      "184   conv2d_50   False\n",
      "185   conv2d_53   False\n",
      "186   batch_normalization_48   False\n",
      "187   batch_normalization_50   False\n",
      "188   batch_normalization_53   False\n",
      "189   activation_48   False\n",
      "190   activation_50   False\n",
      "191   activation_53   False\n",
      "192   block35_7_mixed   False\n",
      "193   block35_7_conv   False\n",
      "194   block35_7   False\n",
      "195   block35_7_ac   False\n",
      "196   conv2d_57   False\n",
      "197   batch_normalization_57   False\n",
      "198   activation_57   False\n",
      "199   conv2d_55   False\n",
      "200   conv2d_58   False\n",
      "201   batch_normalization_55   False\n",
      "202   batch_normalization_58   False\n",
      "203   activation_55   False\n",
      "204   activation_58   False\n",
      "205   conv2d_54   False\n",
      "206   conv2d_56   False\n",
      "207   conv2d_59   False\n",
      "208   batch_normalization_54   False\n",
      "209   batch_normalization_56   False\n",
      "210   batch_normalization_59   False\n",
      "211   activation_54   False\n",
      "212   activation_56   False\n",
      "213   activation_59   False\n",
      "214   block35_8_mixed   False\n",
      "215   block35_8_conv   False\n",
      "216   block35_8   False\n",
      "217   block35_8_ac   False\n",
      "218   conv2d_63   False\n",
      "219   batch_normalization_63   False\n",
      "220   activation_63   False\n",
      "221   conv2d_61   False\n",
      "222   conv2d_64   False\n",
      "223   batch_normalization_61   False\n",
      "224   batch_normalization_64   False\n",
      "225   activation_61   False\n",
      "226   activation_64   False\n",
      "227   conv2d_60   False\n",
      "228   conv2d_62   False\n",
      "229   conv2d_65   False\n",
      "230   batch_normalization_60   False\n",
      "231   batch_normalization_62   False\n",
      "232   batch_normalization_65   False\n",
      "233   activation_60   False\n",
      "234   activation_62   False\n",
      "235   activation_65   False\n",
      "236   block35_9_mixed   False\n",
      "237   block35_9_conv   False\n",
      "238   block35_9   False\n",
      "239   block35_9_ac   False\n",
      "240   conv2d_69   False\n",
      "241   batch_normalization_69   False\n",
      "242   activation_69   False\n",
      "243   conv2d_67   False\n",
      "244   conv2d_70   False\n",
      "245   batch_normalization_67   False\n",
      "246   batch_normalization_70   False\n",
      "247   activation_67   False\n",
      "248   activation_70   False\n",
      "249   conv2d_66   False\n",
      "250   conv2d_68   False\n",
      "251   conv2d_71   False\n",
      "252   batch_normalization_66   False\n",
      "253   batch_normalization_68   False\n",
      "254   batch_normalization_71   False\n",
      "255   activation_66   False\n",
      "256   activation_68   False\n",
      "257   activation_71   False\n",
      "258   block35_10_mixed   False\n",
      "259   block35_10_conv   False\n",
      "260   block35_10   False\n",
      "261   block35_10_ac   False\n",
      "262   conv2d_73   False\n",
      "263   batch_normalization_73   False\n",
      "264   activation_73   False\n",
      "265   conv2d_74   False\n",
      "266   batch_normalization_74   False\n",
      "267   activation_74   False\n",
      "268   conv2d_72   False\n",
      "269   conv2d_75   False\n",
      "270   batch_normalization_72   False\n",
      "271   batch_normalization_75   False\n",
      "272   activation_72   False\n",
      "273   activation_75   False\n",
      "274   max_pooling2d_2   False\n",
      "275   mixed_6a   False\n",
      "276   conv2d_77   False\n",
      "277   batch_normalization_77   False\n",
      "278   activation_77   False\n",
      "279   conv2d_78   False\n",
      "280   batch_normalization_78   False\n",
      "281   activation_78   False\n",
      "282   conv2d_76   False\n",
      "283   conv2d_79   False\n",
      "284   batch_normalization_76   False\n",
      "285   batch_normalization_79   False\n",
      "286   activation_76   False\n",
      "287   activation_79   False\n",
      "288   block17_1_mixed   False\n",
      "289   block17_1_conv   False\n",
      "290   block17_1   False\n",
      "291   block17_1_ac   False\n",
      "292   conv2d_81   False\n",
      "293   batch_normalization_81   False\n",
      "294   activation_81   False\n",
      "295   conv2d_82   False\n",
      "296   batch_normalization_82   False\n",
      "297   activation_82   False\n",
      "298   conv2d_80   False\n",
      "299   conv2d_83   False\n",
      "300   batch_normalization_80   False\n",
      "301   batch_normalization_83   False\n",
      "302   activation_80   False\n",
      "303   activation_83   False\n",
      "304   block17_2_mixed   False\n",
      "305   block17_2_conv   False\n",
      "306   block17_2   False\n",
      "307   block17_2_ac   False\n",
      "308   conv2d_85   False\n",
      "309   batch_normalization_85   False\n",
      "310   activation_85   False\n",
      "311   conv2d_86   False\n",
      "312   batch_normalization_86   False\n",
      "313   activation_86   False\n",
      "314   conv2d_84   False\n",
      "315   conv2d_87   False\n",
      "316   batch_normalization_84   False\n",
      "317   batch_normalization_87   False\n",
      "318   activation_84   False\n",
      "319   activation_87   False\n",
      "320   block17_3_mixed   False\n",
      "321   block17_3_conv   False\n",
      "322   block17_3   False\n",
      "323   block17_3_ac   False\n",
      "324   conv2d_89   False\n",
      "325   batch_normalization_89   False\n",
      "326   activation_89   False\n",
      "327   conv2d_90   False\n",
      "328   batch_normalization_90   False\n",
      "329   activation_90   False\n",
      "330   conv2d_88   False\n",
      "331   conv2d_91   False\n",
      "332   batch_normalization_88   False\n",
      "333   batch_normalization_91   False\n",
      "334   activation_88   False\n",
      "335   activation_91   False\n",
      "336   block17_4_mixed   False\n",
      "337   block17_4_conv   False\n",
      "338   block17_4   False\n",
      "339   block17_4_ac   False\n",
      "340   conv2d_93   False\n",
      "341   batch_normalization_93   False\n",
      "342   activation_93   False\n",
      "343   conv2d_94   False\n",
      "344   batch_normalization_94   False\n",
      "345   activation_94   False\n",
      "346   conv2d_92   False\n",
      "347   conv2d_95   False\n",
      "348   batch_normalization_92   False\n",
      "349   batch_normalization_95   False\n",
      "350   activation_92   False\n",
      "351   activation_95   False\n",
      "352   block17_5_mixed   False\n",
      "353   block17_5_conv   False\n",
      "354   block17_5   False\n",
      "355   block17_5_ac   False\n",
      "356   conv2d_97   False\n",
      "357   batch_normalization_97   False\n",
      "358   activation_97   False\n",
      "359   conv2d_98   False\n",
      "360   batch_normalization_98   False\n",
      "361   activation_98   False\n",
      "362   conv2d_96   False\n",
      "363   conv2d_99   False\n",
      "364   batch_normalization_96   False\n",
      "365   batch_normalization_99   False\n",
      "366   activation_96   False\n",
      "367   activation_99   False\n",
      "368   block17_6_mixed   False\n",
      "369   block17_6_conv   False\n",
      "370   block17_6   False\n",
      "371   block17_6_ac   False\n",
      "372   conv2d_101   False\n",
      "373   batch_normalization_101   False\n",
      "374   activation_101   False\n",
      "375   conv2d_102   False\n",
      "376   batch_normalization_102   False\n",
      "377   activation_102   False\n",
      "378   conv2d_100   False\n",
      "379   conv2d_103   False\n",
      "380   batch_normalization_100   False\n",
      "381   batch_normalization_103   False\n",
      "382   activation_100   False\n",
      "383   activation_103   False\n",
      "384   block17_7_mixed   False\n",
      "385   block17_7_conv   False\n",
      "386   block17_7   False\n",
      "387   block17_7_ac   False\n",
      "388   conv2d_105   False\n",
      "389   batch_normalization_105   False\n",
      "390   activation_105   False\n",
      "391   conv2d_106   False\n",
      "392   batch_normalization_106   False\n",
      "393   activation_106   False\n",
      "394   conv2d_104   False\n",
      "395   conv2d_107   False\n",
      "396   batch_normalization_104   False\n",
      "397   batch_normalization_107   False\n",
      "398   activation_104   False\n",
      "399   activation_107   False\n",
      "400   block17_8_mixed   False\n",
      "401   block17_8_conv   False\n",
      "402   block17_8   False\n",
      "403   block17_8_ac   False\n",
      "404   conv2d_109   False\n",
      "405   batch_normalization_109   False\n",
      "406   activation_109   False\n",
      "407   conv2d_110   False\n",
      "408   batch_normalization_110   False\n",
      "409   activation_110   False\n",
      "410   conv2d_108   False\n",
      "411   conv2d_111   False\n",
      "412   batch_normalization_108   False\n",
      "413   batch_normalization_111   False\n",
      "414   activation_108   False\n",
      "415   activation_111   False\n",
      "416   block17_9_mixed   False\n",
      "417   block17_9_conv   False\n",
      "418   block17_9   False\n",
      "419   block17_9_ac   False\n",
      "420   conv2d_113   False\n",
      "421   batch_normalization_113   False\n",
      "422   activation_113   False\n",
      "423   conv2d_114   False\n",
      "424   batch_normalization_114   False\n",
      "425   activation_114   False\n",
      "426   conv2d_112   False\n",
      "427   conv2d_115   False\n",
      "428   batch_normalization_112   False\n",
      "429   batch_normalization_115   False\n",
      "430   activation_112   False\n",
      "431   activation_115   False\n",
      "432   block17_10_mixed   False\n",
      "433   block17_10_conv   False\n",
      "434   block17_10   False\n",
      "435   block17_10_ac   False\n",
      "436   conv2d_117   False\n",
      "437   batch_normalization_117   False\n",
      "438   activation_117   False\n",
      "439   conv2d_118   False\n",
      "440   batch_normalization_118   False\n",
      "441   activation_118   False\n",
      "442   conv2d_116   False\n",
      "443   conv2d_119   False\n",
      "444   batch_normalization_116   False\n",
      "445   batch_normalization_119   False\n",
      "446   activation_116   False\n",
      "447   activation_119   False\n",
      "448   block17_11_mixed   False\n",
      "449   block17_11_conv   False\n",
      "450   block17_11   False\n",
      "451   block17_11_ac   False\n",
      "452   conv2d_121   False\n",
      "453   batch_normalization_121   False\n",
      "454   activation_121   False\n",
      "455   conv2d_122   False\n",
      "456   batch_normalization_122   False\n",
      "457   activation_122   False\n",
      "458   conv2d_120   False\n",
      "459   conv2d_123   False\n",
      "460   batch_normalization_120   False\n",
      "461   batch_normalization_123   False\n",
      "462   activation_120   False\n",
      "463   activation_123   False\n",
      "464   block17_12_mixed   False\n",
      "465   block17_12_conv   False\n",
      "466   block17_12   False\n",
      "467   block17_12_ac   False\n",
      "468   conv2d_125   False\n",
      "469   batch_normalization_125   False\n",
      "470   activation_125   False\n",
      "471   conv2d_126   False\n",
      "472   batch_normalization_126   False\n",
      "473   activation_126   False\n",
      "474   conv2d_124   False\n",
      "475   conv2d_127   False\n",
      "476   batch_normalization_124   False\n",
      "477   batch_normalization_127   False\n",
      "478   activation_124   False\n",
      "479   activation_127   False\n",
      "480   block17_13_mixed   False\n",
      "481   block17_13_conv   False\n",
      "482   block17_13   False\n",
      "483   block17_13_ac   False\n",
      "484   conv2d_129   False\n",
      "485   batch_normalization_129   False\n",
      "486   activation_129   False\n",
      "487   conv2d_130   False\n",
      "488   batch_normalization_130   False\n",
      "489   activation_130   False\n",
      "490   conv2d_128   False\n",
      "491   conv2d_131   False\n",
      "492   batch_normalization_128   False\n",
      "493   batch_normalization_131   False\n",
      "494   activation_128   False\n",
      "495   activation_131   False\n",
      "496   block17_14_mixed   False\n",
      "497   block17_14_conv   False\n",
      "498   block17_14   False\n",
      "499   block17_14_ac   False\n",
      "500   conv2d_133   False\n",
      "501   batch_normalization_133   False\n",
      "502   activation_133   False\n",
      "503   conv2d_134   False\n",
      "504   batch_normalization_134   False\n",
      "505   activation_134   False\n",
      "506   conv2d_132   False\n",
      "507   conv2d_135   False\n",
      "508   batch_normalization_132   False\n",
      "509   batch_normalization_135   False\n",
      "510   activation_132   False\n",
      "511   activation_135   False\n",
      "512   block17_15_mixed   False\n",
      "513   block17_15_conv   False\n",
      "514   block17_15   False\n",
      "515   block17_15_ac   False\n",
      "516   conv2d_137   False\n",
      "517   batch_normalization_137   False\n",
      "518   activation_137   False\n",
      "519   conv2d_138   False\n",
      "520   batch_normalization_138   False\n",
      "521   activation_138   False\n",
      "522   conv2d_136   False\n",
      "523   conv2d_139   False\n",
      "524   batch_normalization_136   False\n",
      "525   batch_normalization_139   False\n",
      "526   activation_136   False\n",
      "527   activation_139   False\n",
      "528   block17_16_mixed   False\n",
      "529   block17_16_conv   False\n",
      "530   block17_16   False\n",
      "531   block17_16_ac   False\n",
      "532   conv2d_141   False\n",
      "533   batch_normalization_141   False\n",
      "534   activation_141   False\n",
      "535   conv2d_142   False\n",
      "536   batch_normalization_142   False\n",
      "537   activation_142   False\n",
      "538   conv2d_140   False\n",
      "539   conv2d_143   False\n",
      "540   batch_normalization_140   False\n",
      "541   batch_normalization_143   False\n",
      "542   activation_140   False\n",
      "543   activation_143   False\n",
      "544   block17_17_mixed   False\n",
      "545   block17_17_conv   False\n",
      "546   block17_17   False\n",
      "547   block17_17_ac   False\n",
      "548   conv2d_145   False\n",
      "549   batch_normalization_145   False\n",
      "550   activation_145   False\n",
      "551   conv2d_146   False\n",
      "552   batch_normalization_146   False\n",
      "553   activation_146   False\n",
      "554   conv2d_144   False\n",
      "555   conv2d_147   False\n",
      "556   batch_normalization_144   False\n",
      "557   batch_normalization_147   False\n",
      "558   activation_144   False\n",
      "559   activation_147   False\n",
      "560   block17_18_mixed   False\n",
      "561   block17_18_conv   False\n",
      "562   block17_18   False\n",
      "563   block17_18_ac   False\n",
      "564   conv2d_149   False\n",
      "565   batch_normalization_149   False\n",
      "566   activation_149   False\n",
      "567   conv2d_150   False\n",
      "568   batch_normalization_150   False\n",
      "569   activation_150   False\n",
      "570   conv2d_148   False\n",
      "571   conv2d_151   False\n",
      "572   batch_normalization_148   False\n",
      "573   batch_normalization_151   False\n",
      "574   activation_148   False\n",
      "575   activation_151   False\n",
      "576   block17_19_mixed   False\n",
      "577   block17_19_conv   False\n",
      "578   block17_19   False\n",
      "579   block17_19_ac   False\n",
      "580   conv2d_153   False\n",
      "581   batch_normalization_153   False\n",
      "582   activation_153   False\n",
      "583   conv2d_154   False\n",
      "584   batch_normalization_154   False\n",
      "585   activation_154   False\n",
      "586   conv2d_152   False\n",
      "587   conv2d_155   False\n",
      "588   batch_normalization_152   False\n",
      "589   batch_normalization_155   False\n",
      "590   activation_152   False\n",
      "591   activation_155   False\n",
      "592   block17_20_mixed   False\n",
      "593   block17_20_conv   False\n",
      "594   block17_20   False\n",
      "595   block17_20_ac   False\n",
      "596   conv2d_160   False\n",
      "597   batch_normalization_160   False\n",
      "598   activation_160   False\n",
      "599   conv2d_156   False\n",
      "600   conv2d_158   False\n",
      "601   conv2d_161   False\n",
      "602   batch_normalization_156   False\n",
      "603   batch_normalization_158   False\n",
      "604   batch_normalization_161   False\n",
      "605   activation_156   False\n",
      "606   activation_158   False\n",
      "607   activation_161   False\n",
      "608   conv2d_157   False\n",
      "609   conv2d_159   False\n",
      "610   conv2d_162   False\n",
      "611   batch_normalization_157   False\n",
      "612   batch_normalization_159   False\n",
      "613   batch_normalization_162   False\n",
      "614   activation_157   False\n",
      "615   activation_159   False\n",
      "616   activation_162   False\n",
      "617   max_pooling2d_3   False\n",
      "618   mixed_7a   False\n",
      "619   conv2d_164   False\n",
      "620   batch_normalization_164   False\n",
      "621   activation_164   False\n",
      "622   conv2d_165   False\n",
      "623   batch_normalization_165   False\n",
      "624   activation_165   False\n",
      "625   conv2d_163   False\n",
      "626   conv2d_166   False\n",
      "627   batch_normalization_163   False\n",
      "628   batch_normalization_166   False\n",
      "629   activation_163   False\n",
      "630   activation_166   False\n",
      "631   block8_1_mixed   False\n",
      "632   block8_1_conv   False\n",
      "633   block8_1   False\n",
      "634   block8_1_ac   False\n",
      "635   conv2d_168   False\n",
      "636   batch_normalization_168   False\n",
      "637   activation_168   False\n",
      "638   conv2d_169   False\n",
      "639   batch_normalization_169   False\n",
      "640   activation_169   False\n",
      "641   conv2d_167   False\n",
      "642   conv2d_170   False\n",
      "643   batch_normalization_167   False\n",
      "644   batch_normalization_170   False\n",
      "645   activation_167   False\n",
      "646   activation_170   False\n",
      "647   block8_2_mixed   False\n",
      "648   block8_2_conv   False\n",
      "649   block8_2   False\n",
      "650   block8_2_ac   False\n",
      "651   conv2d_172   False\n",
      "652   batch_normalization_172   False\n",
      "653   activation_172   False\n",
      "654   conv2d_173   False\n",
      "655   batch_normalization_173   False\n",
      "656   activation_173   False\n",
      "657   conv2d_171   False\n",
      "658   conv2d_174   False\n",
      "659   batch_normalization_171   False\n",
      "660   batch_normalization_174   False\n",
      "661   activation_171   False\n",
      "662   activation_174   False\n",
      "663   block8_3_mixed   False\n",
      "664   block8_3_conv   False\n",
      "665   block8_3   False\n",
      "666   block8_3_ac   False\n",
      "667   conv2d_176   False\n",
      "668   batch_normalization_176   False\n",
      "669   activation_176   False\n",
      "670   conv2d_177   False\n",
      "671   batch_normalization_177   False\n",
      "672   activation_177   False\n",
      "673   conv2d_175   False\n",
      "674   conv2d_178   False\n",
      "675   batch_normalization_175   False\n",
      "676   batch_normalization_178   False\n",
      "677   activation_175   False\n",
      "678   activation_178   False\n",
      "679   block8_4_mixed   False\n",
      "680   block8_4_conv   False\n",
      "681   block8_4   False\n",
      "682   block8_4_ac   False\n",
      "683   conv2d_180   False\n",
      "684   batch_normalization_180   False\n",
      "685   activation_180   False\n",
      "686   conv2d_181   False\n",
      "687   batch_normalization_181   False\n",
      "688   activation_181   False\n",
      "689   conv2d_179   False\n",
      "690   conv2d_182   False\n",
      "691   batch_normalization_179   False\n",
      "692   batch_normalization_182   False\n",
      "693   activation_179   False\n",
      "694   activation_182   False\n",
      "695   block8_5_mixed   False\n",
      "696   block8_5_conv   False\n",
      "697   block8_5   False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698   block8_5_ac   False\n",
      "699   conv2d_184   False\n",
      "700   batch_normalization_184   False\n",
      "701   activation_184   False\n",
      "702   conv2d_185   False\n",
      "703   batch_normalization_185   False\n",
      "704   activation_185   False\n",
      "705   conv2d_183   False\n",
      "706   conv2d_186   False\n",
      "707   batch_normalization_183   False\n",
      "708   batch_normalization_186   False\n",
      "709   activation_183   False\n",
      "710   activation_186   False\n",
      "711   block8_6_mixed   True\n",
      "712   block8_6_conv   True\n",
      "713   block8_6   True\n",
      "714   block8_6_ac   True\n",
      "715   conv2d_188   True\n",
      "716   batch_normalization_188   True\n",
      "717   activation_188   True\n",
      "718   conv2d_189   True\n",
      "719   batch_normalization_189   True\n",
      "720   activation_189   True\n",
      "721   conv2d_187   True\n",
      "722   conv2d_190   True\n",
      "723   batch_normalization_187   True\n",
      "724   batch_normalization_190   True\n",
      "725   activation_187   True\n",
      "726   activation_190   True\n",
      "727   block8_7_mixed   True\n",
      "728   block8_7_conv   True\n",
      "729   block8_7   True\n",
      "730   block8_7_ac   True\n",
      "731   conv2d_192   True\n",
      "732   batch_normalization_192   True\n",
      "733   activation_192   True\n",
      "734   conv2d_193   True\n",
      "735   batch_normalization_193   True\n",
      "736   activation_193   True\n",
      "737   conv2d_191   True\n",
      "738   conv2d_194   True\n",
      "739   batch_normalization_191   True\n",
      "740   batch_normalization_194   True\n",
      "741   activation_191   True\n",
      "742   activation_194   True\n",
      "743   block8_8_mixed   True\n",
      "744   block8_8_conv   True\n",
      "745   block8_8   True\n",
      "746   block8_8_ac   True\n",
      "747   conv2d_196   True\n",
      "748   batch_normalization_196   True\n",
      "749   activation_196   True\n",
      "750   conv2d_197   True\n",
      "751   batch_normalization_197   True\n",
      "752   activation_197   True\n",
      "753   conv2d_195   True\n",
      "754   conv2d_198   True\n",
      "755   batch_normalization_195   True\n",
      "756   batch_normalization_198   True\n",
      "757   activation_195   True\n",
      "758   activation_198   True\n",
      "759   block8_9_mixed   True\n",
      "760   block8_9_conv   True\n",
      "761   block8_9   True\n",
      "762   block8_9_ac   True\n",
      "763   conv2d_200   True\n",
      "764   batch_normalization_200   True\n",
      "765   activation_200   True\n",
      "766   conv2d_201   True\n",
      "767   batch_normalization_201   True\n",
      "768   activation_201   True\n",
      "769   conv2d_199   True\n",
      "770   conv2d_202   True\n",
      "771   batch_normalization_199   True\n",
      "772   batch_normalization_202   True\n",
      "773   activation_199   True\n",
      "774   activation_202   True\n",
      "775   block8_10_mixed   True\n",
      "776   block8_10_conv   True\n",
      "777   block8_10   True\n",
      "778   conv_7b   True\n",
      "779   conv_7b_bn   True\n",
      "780   conv_7b_ac   True\n",
      "model retrieved\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_resnet_v2 (Functio (None, 5, 5, 1536)        54336736  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 38400)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              39322624  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 95,768,810\n",
      "Trainable params: 53,700,650\n",
      "Non-trainable params: 42,068,160\n",
      "_________________________________________________________________\n",
      "model building done\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 786s 10s/step - loss: 5.6936 - accuracy: 0.3540 - val_loss: 78.7704 - val_accuracy: 0.3625\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 782s 10s/step - loss: 1.0643 - accuracy: 0.7164 - val_loss: 1.1612 - val_accuracy: 0.7484\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 808s 10s/step - loss: 0.7028 - accuracy: 0.7874 - val_loss: 0.7121 - val_accuracy: 0.8000\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 778s 10s/step - loss: 0.6340 - accuracy: 0.8014 - val_loss: 0.8214 - val_accuracy: 0.7703\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 852s 11s/step - loss: 0.5337 - accuracy: 0.8365 - val_loss: 0.7486 - val_accuracy: 0.7844\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 958s 12s/step - loss: 0.4807 - accuracy: 0.8504 - val_loss: 0.9082 - val_accuracy: 0.7609\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 887s 11s/step - loss: 0.4228 - accuracy: 0.8668 - val_loss: 0.9027 - val_accuracy: 0.7641\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 838s 10s/step - loss: 0.4113 - accuracy: 0.8661 - val_loss: 0.8920 - val_accuracy: 0.7719\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 878s 11s/step - loss: 0.3695 - accuracy: 0.8843 - val_loss: 0.8467 - val_accuracy: 0.7844\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 993s 12s/step - loss: 0.3074 - accuracy: 0.9037 - val_loss: 0.8193 - val_accuracy: 0.8109\n",
      "32/32 [==============================] - 347s 11s/step - loss: 0.8973 - accuracy: 0.7805\n",
      "model training done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2794<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210413_094342-oicnhuuu/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210413_094342-oicnhuuu/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.34125</td></tr><tr><td>accuracy</td><td>0.89414</td></tr><tr><td>val_loss</td><td>0.81931</td></tr><tr><td>val_accuracy</td><td>0.81094</td></tr><tr><td>_runtime</td><td>8927</td></tr><tr><td>_timestamp</td><td>1618296149</td></tr><tr><td>_step</td><td>10</td></tr><tr><td>best_val_accuracy</td><td>0.81094</td></tr><tr><td>best_epoch</td><td>9</td></tr><tr><td>test_acc </td><td>0.7805</td></tr><tr><td>test_loss </td><td>0.89731</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▃▂▂▂▂▁▁▁▁</td></tr><tr><td>accuracy</td><td>▁▅▆▆▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▇█▇█▇▇▇██</td></tr><tr><td>_runtime</td><td>▁▂▂▃▄▅▅▆▇██</td></tr><tr><td>_timestamp</td><td>▁▂▂▃▄▅▅▆▇██</td></tr><tr><td>_step</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>test_acc </td><td>▁</td></tr><tr><td>test_loss </td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">cs6910-a2-partB</strong>: <a href=\"https://wandb.ai/notarchana/cs6910-a2/runs/oicnhuuu\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/runs/oicnhuuu</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6tpyee38 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: ['VGG16', 18, 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.25<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">cs6910-a2-partB</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/notarchana/cs6910-a2\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/notarchana/cs6910-a2/sweeps/wyy702q1\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/sweeps/wyy702q1</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/notarchana/cs6910-a2/runs/6tpyee38\" target=\"_blank\">https://wandb.ai/notarchana/cs6910-a2/runs/6tpyee38</a><br/>\n",
       "                Run data is saved locally in <code>/home/archana/acads/sem6/dl/a2/cs6910-a2-PartB/wandb/run-20210413_121413-6tpyee38</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "1   input_1   False\n",
      "2   block1_conv1   False\n",
      "3   block1_conv2   False\n",
      "4   block1_pool   False\n",
      "5   block2_conv1   False\n",
      "6   block2_conv2   False\n",
      "7   block2_pool   False\n",
      "8   block3_conv1   False\n",
      "9   block3_conv2   False\n",
      "10   block3_conv3   False\n",
      "11   block3_pool   False\n",
      "12   block4_conv1   False\n",
      "13   block4_conv2   False\n",
      "14   block4_conv3   False\n",
      "15   block4_pool   False\n",
      "16   block5_conv1   False\n",
      "17   block5_conv2   False\n",
      "18   block5_conv3   False\n",
      "19   block5_pool   True\n",
      "model retrieved\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 42,515,274\n",
      "Trainable params: 27,800,586\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "model building done\n",
      "Epoch 1/10\n",
      "42/80 [==============>...............] - ETA: 6:51 - loss: 4.1966 - accuracy: 0.1435"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97072f59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
